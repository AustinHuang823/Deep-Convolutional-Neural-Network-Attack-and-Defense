{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05114a43",
   "metadata": {},
   "source": [
    "## EN 520.665 Machine Perception Project 1\n",
    "#### Yu-Chun(Arthur) Ku, Ching-Yang(Austin) Huang\n",
    "##### yck4@jh.edu, chuan120@jh.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cdd7c2",
   "metadata": {},
   "source": [
    "### AlexNet CIFAR-10 model training and attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f9db4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "import argparse\n",
    "\n",
    "from AlexNet import AlexNet\n",
    "from misc import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3f0bb",
   "metadata": {},
   "source": [
    "Here we import AlexNet to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cfb882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a605f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    def __init__(self, config):\n",
    "        self.model = None\n",
    "        self.lr = config.lr\n",
    "        self.epochs = config.epoch\n",
    "        self.train_batch_size = config.trainBatchSize\n",
    "        self.test_batch_size = config.testBatchSize\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.device = None\n",
    "        self.train_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    def load_data(self):\n",
    "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=self.train_batch_size, shuffle=True)\n",
    "        test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=self.test_batch_size, shuffle=False)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.device = torch.device(\"mps\")\n",
    "\n",
    "        self.model = AlexNet().to(self.device)\n",
    "        #self.model = GoogLeNet().to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[75, 150], gamma=0.5)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"train:\")\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_num, (data, target) in enumerate(self.train_loader):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            prediction = torch.max(output, 1)  # second param \"1\" represents the dimension to be reduced\n",
    "            total += target.size(0)\n",
    "\n",
    "            # train_correct incremented by one if predicted right\n",
    "            train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
    "\n",
    "            progress_bar(batch_num, len(self.train_loader), 'Loss: %.4f | Acc: %.3f%% (%d/%d)'\n",
    "                         % (train_loss / (batch_num + 1), 100. * train_correct / total, train_correct, total))\n",
    "\n",
    "        return train_loss, train_correct / total\n",
    "\n",
    "    def test(self):\n",
    "        print(\"test:\")\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        test_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_num, (data, target) in enumerate(self.test_loader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                test_loss += loss.item()\n",
    "                prediction = torch.max(output, 1)\n",
    "                total += target.size(0)\n",
    "                test_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
    "\n",
    "                progress_bar(batch_num, len(self.test_loader), 'Loss: %.4f | Acc: %.3f%% (%d/%d)'\n",
    "                             % (test_loss / (batch_num + 1), 100. * test_correct / total, test_correct, total))\n",
    "\n",
    "        return test_loss, test_correct / total\n",
    "\n",
    "    def save(self):\n",
    "        model_out_path = \"model.pth\"\n",
    "        torch.save(self.model, model_out_path)\n",
    "        print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "\n",
    "    def run(self):\n",
    "        self.load_data()\n",
    "        self.load_model()\n",
    "        accuracy = 0\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(\"\\n===> epoch: %d/100\" % epoch)\n",
    "            train_result = self.train()\n",
    "            print(train_result)\n",
    "            self.scheduler.step()\n",
    "        test_result = self.test()\n",
    "        accuracy = max(accuracy, test_result[1])\n",
    "        print(\"===> BEST ACC. PERFORMANCE: %.3f%%\" % (accuracy * 100))\n",
    "        self.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11b1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"cifar-10 with PyTorch\")\n",
    "# parser.add_argument('--lr', default=0.001, type=float, help='learning rate')\n",
    "# parser.add_argument('--epoch', default=200, type=int, help='number of epochs tp train for')\n",
    "# parser.add_argument('--trainBatchSize', default=100, type=int, help='training batch size')\n",
    "# parser.add_argument('--testBatchSize', default=100, type=int, help='testing batch size')\n",
    "def main():\n",
    "    args = edict({\n",
    "        \"lr\": 0.001,\n",
    "        \"epoch\": 100,\n",
    "        \"trainBatchSize\": 100,\n",
    "        \"testBatchSize\": 100\n",
    "    })\n",
    "\n",
    "\n",
    "    solver = Solver(args)\n",
    "    solver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b835759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "===> epoch: 1/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s665ms | Loss: 1.7518 | Acc: 32.428% (16214/50000)========>....................................................................]  Step: 66ms | Tot: 5s60ms | Loss: 2.1873 | Acc: 14.514% (1074/7400) [===============>................................................................]  Step: 65ms | Tot: 6s669ms | Loss: 2.1417 | Acc: 15.847% (1553/9800) [=================>..............................................................]  Step: 66ms | Tot: 7s267ms | Loss: 2.1258 | Acc: 16.327% (1747/10700) [====================>...........................................................]  Step: 66ms | Tot: 8s646ms | Loss: 2.0951 | Acc: 17.307% (2198/12700) [=====================>..........................................................]  Step: 68ms | Tot: 9s189ms | Loss: 2.0810 | Acc: 17.822% (2406/13500) [======================>.........................................................]  Step: 67ms | Tot: 9s662ms | Loss: 2.0687 | Acc: 18.331% (2603/14200) [==========================>.....................................................]  Step: 66ms | Tot: 11s162ms | Loss: 2.0390 | Acc: 19.497% (3178/16300) [===============================>................................................]  Step: 66ms | Tot: 13s563ms | Loss: 1.9910 | Acc: 21.844% (4347/19900) [================================>...............................................]  Step: 66ms | Tot: 13s763ms | Loss: 1.9877 | Acc: 21.985% (4441/20200) [=====================================>..........................................]  Step: 66ms | Tot: 15s820ms | Loss: 1.9547 | Acc: 23.442% (5462/23300) [=====================================>..........................................]  Step: 67ms | Tot: 15s888ms | Loss: 1.9534 | Acc: 23.466% (5491/23400) [=====================================================>..........................]  Step: 67ms | Tot: 22s650ms | Loss: 1.8570 | Acc: 27.725% (9288/33500) [======================================================>.........................]  Step: 68ms | Tot: 22s987ms | Loss: 1.8517 | Acc: 27.950% (9503/34000) [======================================================>.........................]  Step: 66ms | Tot: 23s187ms | Loss: 1.8499 | Acc: 28.015% (9609/34300) [=======================================================>........................]  Step: 66ms | Tot: 23s388ms | Loss: 1.8477 | Acc: 28.121% (9730/34600) [==============================================================>.................]  Step: 67ms | Tot: 26s182ms | Loss: 1.8198 | Acc: 29.374% (11397/38800) [==============================================================>.................]  Step: 67ms | Tot: 26s249ms | Loss: 1.8193 | Acc: 29.414% (11442/38900) [=================================================================>..............]  Step: 66ms | Tot: 27s647ms | Loss: 1.8053 | Acc: 30.039% (12316/41000)\n",
      "(875.8766356706619, 0.32428)\n",
      "\n",
      "===> epoch: 2/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s226ms | Loss: 1.3525 | Acc: 50.546% (25273/50000)============>................................................................]  Step: 67ms | Tot: 6s373ms | Loss: 1.4346 | Acc: 46.577% (4518/9700) [================>...............................................................]  Step: 66ms | Tot: 6s575ms | Loss: 1.4341 | Acc: 46.540% (4654/10000) [==================>.............................................................]  Step: 66ms | Tot: 7s573ms | Loss: 1.4391 | Acc: 46.487% (5346/11500) [==================>.............................................................]  Step: 66ms | Tot: 7s774ms | Loss: 1.4381 | Acc: 46.525% (5490/11800) [===================>............................................................]  Step: 66ms | Tot: 8s108ms | Loss: 1.4373 | Acc: 46.569% (5728/12300) [=============================>..................................................]  Step: 67ms | Tot: 12s141ms | Loss: 1.4318 | Acc: 47.049% (8610/18300) [=================================>..............................................]  Step: 66ms | Tot: 14s68ms | Loss: 1.4227 | Acc: 47.547% (10080/21200) [==================================>.............................................]  Step: 66ms | Tot: 14s202ms | Loss: 1.4228 | Acc: 47.533% (10172/21400) [=====================================================================>..........]  Step: 67ms | Tot: 28s711ms | Loss: 1.3675 | Acc: 50.005% (21652/43300) [========================================================================>.......]  Step: 66ms | Tot: 30s90ms | Loss: 1.3643 | Acc: 50.115% (22702/45300) [===============================================================================>]  Step: 67ms | Tot: 32s957ms | Loss: 1.3535 | Acc: 50.512% (25054/49600) [===============================================================================>]  Step: 66ms | Tot: 33s90ms | Loss: 1.3531 | Acc: 50.526% (25162/49800)\n",
      "(676.2587625384331, 0.50546)\n",
      "\n",
      "===> epoch: 3/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 68ms | Tot: 33s323ms | Loss: 1.1938 | Acc: 56.966% (28483/50000).....................................................................]  Step: 68ms | Tot: 68ms | Loss: 1.0138 | Acc: 66.000% (132/200) [=====>..........................................................................]  Step: 67ms | Tot: 2s268ms | Loss: 1.2092 | Acc: 56.143% (1965/3500) [=========>......................................................................]  Step: 66ms | Tot: 3s801ms | Loss: 1.2064 | Acc: 55.534% (3221/5800) [===================>............................................................]  Step: 66ms | Tot: 8s33ms | Loss: 1.2157 | Acc: 55.678% (6737/12100) [======================================>.........................................]  Step: 67ms | Tot: 16s123ms | Loss: 1.2069 | Acc: 56.062% (13623/24300) [========================================>.......................................]  Step: 67ms | Tot: 16s665ms | Loss: 1.2083 | Acc: 56.016% (14060/25100) [=========================================>......................................]  Step: 66ms | Tot: 17s68ms | Loss: 1.2080 | Acc: 56.062% (14408/25700) [=============================================>..................................]  Step: 66ms | Tot: 18s871ms | Loss: 1.2047 | Acc: 56.299% (15989/28400) [===============================================>................................]  Step: 67ms | Tot: 19s604ms | Loss: 1.2041 | Acc: 56.298% (16608/29500) [===================================================>............................]  Step: 66ms | Tot: 21s468ms | Loss: 1.2028 | Acc: 56.362% (18205/32300) [====================================================>...........................]  Step: 66ms | Tot: 21s936ms | Loss: 1.2005 | Acc: 56.400% (18612/33000) [===========================================================>....................]  Step: 67ms | Tot: 24s795ms | Loss: 1.2012 | Acc: 56.469% (21063/37300) [===========================================================>....................]  Step: 66ms | Tot: 24s862ms | Loss: 1.2012 | Acc: 56.489% (21127/37400) [============================================================>...................]  Step: 67ms | Tot: 24s997ms | Loss: 1.2014 | Acc: 56.487% (21239/37600) [===================================================================>............]  Step: 66ms | Tot: 27s974ms | Loss: 1.2008 | Acc: 56.550% (23751/42000)\n",
      "(596.9194981455803, 0.56966)\n",
      "\n",
      "===> epoch: 4/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s67ms | Loss: 1.0883 | Acc: 61.602% (30801/50000)==>...........................................................................]  Step: 67ms | Tot: 1s607ms | Loss: 1.1179 | Acc: 59.720% (1493/2500) [======>.........................................................................]  Step: 66ms | Tot: 2s471ms | Loss: 1.1027 | Acc: 60.789% (2310/3800) [===========>....................................................................]  Step: 66ms | Tot: 4s546ms | Loss: 1.0899 | Acc: 61.101% (4216/6900) [===========>....................................................................]  Step: 66ms | Tot: 4s747ms | Loss: 1.0919 | Acc: 61.125% (4401/7200) [===========>....................................................................]  Step: 67ms | Tot: 4s815ms | Loss: 1.0893 | Acc: 61.192% (4467/7300) [================================>...............................................]  Step: 66ms | Tot: 13s399ms | Loss: 1.1017 | Acc: 60.946% (12372/20300) [==================================>.............................................]  Step: 66ms | Tot: 14s333ms | Loss: 1.1039 | Acc: 60.926% (13221/21700) [=======================================>........................................]  Step: 68ms | Tot: 16s446ms | Loss: 1.1016 | Acc: 61.024% (15195/24900) [===========================================================================>....]  Step: 66ms | Tot: 31s18ms | Loss: 1.0918 | Acc: 61.439% (28815/46900) [===========================================================================>....]  Step: 67ms | Tot: 31s153ms | Loss: 1.0918 | Acc: 61.442% (28939/47100)\n",
      "(544.1446048021317, 0.61602)\n",
      "\n",
      "===> epoch: 5/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s451ms | Loss: 1.0056 | Acc: 64.810% (32405/50000).......................................................................]  Step: 66ms | Tot: 132ms | Loss: 1.0873 | Acc: 63.333% (190/300) [================>...............................................................]  Step: 66ms | Tot: 6s838ms | Loss: 1.0134 | Acc: 64.288% (6686/10400) [===================>............................................................]  Step: 66ms | Tot: 8s116ms | Loss: 1.0160 | Acc: 64.138% (7889/12300) [=======================================>........................................]  Step: 65ms | Tot: 16s482ms | Loss: 1.0139 | Acc: 64.377% (15901/24700) [====================================================>...........................]  Step: 66ms | Tot: 21s933ms | Loss: 1.0124 | Acc: 64.546% (21171/32800) [====================================================>...........................]  Step: 67ms | Tot: 22s134ms | Loss: 1.0125 | Acc: 64.547% (21365/33100) [======================================================>.........................]  Step: 66ms | Tot: 22s602ms | Loss: 1.0136 | Acc: 64.444% (21782/33800) [============================================================>...................]  Step: 69ms | Tot: 25s327ms | Loss: 1.0116 | Acc: 64.444% (24360/37800) [===============================================================================>]  Step: 66ms | Tot: 33s385ms | Loss: 1.0052 | Acc: 64.824% (32347/49900)\n",
      "(502.8025954961777, 0.6481)\n",
      "\n",
      "===> epoch: 6/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s933ms | Loss: 0.9432 | Acc: 67.118% (33559/50000)[=========================================>......................................]  Step: 68ms | Tot: 17s34ms | Loss: 0.9453 | Acc: 66.458% (17279/26000) [=============================================>..................................]  Step: 67ms | Tot: 18s633ms | Loss: 0.9456 | Acc: 66.521% (18892/28400) [=============================================>..................................]  Step: 67ms | Tot: 18s700ms | Loss: 0.9468 | Acc: 66.505% (18954/28500) [====================================================>...........................]  Step: 67ms | Tot: 21s401ms | Loss: 0.9471 | Acc: 66.646% (21660/32500) [=====================================================>..........................]  Step: 66ms | Tot: 22s201ms | Loss: 0.9433 | Acc: 66.819% (22518/33700) [========================================================>.......................]  Step: 69ms | Tot: 23s254ms | Loss: 0.9425 | Acc: 66.836% (23593/35300) [===========================================================>....................]  Step: 67ms | Tot: 24s312ms | Loss: 0.9427 | Acc: 66.816% (24655/36900) [===============================================================>................]  Step: 66ms | Tot: 26s96ms | Loss: 0.9434 | Acc: 66.934% (26506/39600)\n",
      "(471.60310155153275, 0.67118)\n",
      "\n",
      "===> epoch: 7/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s906ms | Loss: 0.8902 | Acc: 69.082% (34541/50000)[===========================================================>....................]  Step: 67ms | Tot: 24s348ms | Loss: 0.8954 | Acc: 68.770% (25445/37000) [=======================================================================>........]  Step: 67ms | Tot: 29s214ms | Loss: 0.8927 | Acc: 68.896% (30590/44400)\n",
      "(445.0776973962784, 0.69082)\n",
      "\n",
      "===> epoch: 8/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s201ms | Loss: 0.8356 | Acc: 71.348% (35674/50000)........................................................................]  Step: 67ms | Tot: 607ms | Loss: 0.8403 | Acc: 69.800% (698/1000) [===========================>....................................................]  Step: 67ms | Tot: 11s429ms | Loss: 0.8394 | Acc: 71.387% (12350/17300) [======================================>.........................................]  Step: 69ms | Tot: 15s711ms | Loss: 0.8373 | Acc: 71.450% (17005/23800) [=========================================>......................................]  Step: 68ms | Tot: 17s235ms | Loss: 0.8366 | Acc: 71.375% (18629/26100) [=============================================>..................................]  Step: 65ms | Tot: 18s753ms | Loss: 0.8364 | Acc: 71.389% (20203/28300) [=============================================>..................................]  Step: 67ms | Tot: 18s955ms | Loss: 0.8366 | Acc: 71.392% (20418/28600) [=============================================>..................................]  Step: 66ms | Tot: 19s21ms | Loss: 0.8370 | Acc: 71.369% (20483/28700)\n",
      "(417.8217519521713, 0.71348)\n",
      "\n",
      "===> epoch: 9/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s108ms | Loss: 0.8028 | Acc: 72.344% (36172/50000)==========>..................................................................]  Step: 68ms | Tot: 5s547ms | Loss: 0.7991 | Acc: 72.143% (6060/8400) [==================================================================>.............]  Step: 66ms | Tot: 27s487ms | Loss: 0.8034 | Acc: 72.288% (30072/41600) [=======================================================================>........]  Step: 67ms | Tot: 29s487ms | Loss: 0.8028 | Acc: 72.292% (32170/44500)\n",
      "(401.4009482860565, 0.72344)\n",
      "\n",
      "===> epoch: 10/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s53ms | Loss: 0.7548 | Acc: 74.026% (37013/50000) [========================================================>.......................]  Step: 67ms | Tot: 23s325ms | Loss: 0.7489 | Acc: 74.243% (26282/35400) [=========================================================>......................]  Step: 67ms | Tot: 23s865ms | Loss: 0.7486 | Acc: 74.262% (26883/36200) [============================================================>...................]  Step: 66ms | Tot: 24s845ms | Loss: 0.7497 | Acc: 74.207% (27902/37600) [=============================================================>..................]  Step: 66ms | Tot: 25s383ms | Loss: 0.7496 | Acc: 74.229% (28504/38400)\n",
      "(377.3874378502369, 0.74026)\n",
      "\n",
      "===> epoch: 11/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s183ms | Loss: 0.7299 | Acc: 74.818% (37409/50000)====>........................................................................]  Step: 67ms | Tot: 2s910ms | Loss: 0.7244 | Acc: 75.523% (3323/4400) [=======>........................................................................]  Step: 66ms | Tot: 2s976ms | Loss: 0.7235 | Acc: 75.600% (3402/4500) [==========>.....................................................................]  Step: 68ms | Tot: 4s231ms | Loss: 0.7336 | Acc: 75.156% (4810/6400) [==========>.....................................................................]  Step: 66ms | Tot: 4s364ms | Loss: 0.7313 | Acc: 75.152% (4960/6600) [===========>....................................................................]  Step: 66ms | Tot: 4s632ms | Loss: 0.7280 | Acc: 75.214% (5265/7000) [=================>..............................................................]  Step: 67ms | Tot: 7s328ms | Loss: 0.7345 | Acc: 74.855% (8234/11000) [=================>..............................................................]  Step: 67ms | Tot: 7s395ms | Loss: 0.7324 | Acc: 74.919% (8316/11100) [==================>.............................................................]  Step: 66ms | Tot: 7s662ms | Loss: 0.7307 | Acc: 74.939% (8618/11500) [=========================================>......................................]  Step: 68ms | Tot: 17s254ms | Loss: 0.7282 | Acc: 74.888% (19396/25900) [==================================================>.............................]  Step: 67ms | Tot: 20s876ms | Loss: 0.7260 | Acc: 74.965% (23464/31300) [===================================================>............................]  Step: 66ms | Tot: 21s281ms | Loss: 0.7248 | Acc: 75.025% (23933/31900) [==================================================================>.............]  Step: 66ms | Tot: 27s798ms | Loss: 0.7233 | Acc: 75.067% (31378/41800)\n",
      "(364.9504527449608, 0.74818)\n",
      "\n",
      "===> epoch: 12/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s77ms | Loss: 0.6974 | Acc: 76.114% (38057/50000)===========>..................................................................]  Step: 67ms | Tot: 5s640ms | Loss: 0.6887 | Acc: 76.221% (6555/8600) [==================>.............................................................]  Step: 70ms | Tot: 7s559ms | Loss: 0.6866 | Acc: 76.409% (8787/11500) [======================>.........................................................]  Step: 66ms | Tot: 9s345ms | Loss: 0.6880 | Acc: 76.390% (10771/14100) [=======================>........................................................]  Step: 67ms | Tot: 9s546ms | Loss: 0.6873 | Acc: 76.424% (11005/14400) [======================================================>.........................]  Step: 66ms | Tot: 22s430ms | Loss: 0.6911 | Acc: 76.295% (25864/33900) [=======================================================================>........]  Step: 66ms | Tot: 29s646ms | Loss: 0.6967 | Acc: 76.143% (34112/44800)\n",
      "(348.71546629071236, 0.76114)\n",
      "\n",
      "===> epoch: 13/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s105ms | Loss: 0.6787 | Acc: 76.576% (38288/50000)[===================================>............................................]  Step: 66ms | Tot: 14s580ms | Loss: 0.6613 | Acc: 76.982% (16859/21900) [===================================>............................................]  Step: 67ms | Tot: 14s647ms | Loss: 0.6624 | Acc: 76.977% (16935/22000) [====================================>...........................................]  Step: 66ms | Tot: 15s113ms | Loss: 0.6619 | Acc: 76.982% (17475/22700)\n",
      "(339.3640378713608, 0.76576)\n",
      "\n",
      "===> epoch: 14/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s835ms | Loss: 0.6482 | Acc: 77.696% (38848/50000)[=================================>..............................................]  Step: 66ms | Tot: 13s729ms | Loss: 0.6329 | Acc: 78.148% (16333/20900)\n",
      "(324.0982258319855, 0.77696)\n",
      "\n",
      "===> epoch: 15/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 68ms | Tot: 33s298ms | Loss: 0.6259 | Acc: 78.324% (39162/50000)========================>.......................................................]  Step: 66ms | Tot: 9s889ms | Loss: 0.6089 | Acc: 79.033% (11934/15100) [========================>.......................................................]  Step: 67ms | Tot: 9s957ms | Loss: 0.6090 | Acc: 79.033% (12013/15200) [=========================>......................................................]  Step: 67ms | Tot: 10s627ms | Loss: 0.6138 | Acc: 78.883% (12779/16200) [================================>...............................................]  Step: 66ms | Tot: 13s363ms | Loss: 0.6225 | Acc: 78.505% (15858/20200) [================================================================>...............]  Step: 66ms | Tot: 26s852ms | Loss: 0.6228 | Acc: 78.484% (31786/40500) [========================================================================>.......]  Step: 68ms | Tot: 29s913ms | Loss: 0.6223 | Acc: 78.451% (35303/45000) [==============================================================================>.]  Step: 66ms | Tot: 32s752ms | Loss: 0.6255 | Acc: 78.362% (38554/49200) [===============================================================================>]  Step: 68ms | Tot: 32s888ms | Loss: 0.6260 | Acc: 78.346% (38703/49400)\n",
      "(312.95813179016113, 0.78324)\n",
      "\n",
      "===> epoch: 16/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 72ms | Tot: 33s489ms | Loss: 0.6040 | Acc: 79.084% (39542/50000)=====>.......................................................................]  Step: 67ms | Tot: 3s718ms | Loss: 0.5956 | Acc: 79.464% (4450/5600) [=========>......................................................................]  Step: 65ms | Tot: 4s53ms | Loss: 0.5961 | Acc: 79.393% (4843/6100) [==========================>.....................................................]  Step: 68ms | Tot: 10s949ms | Loss: 0.6027 | Acc: 79.165% (12983/16400) [===========================>....................................................]  Step: 68ms | Tot: 11s635ms | Loss: 0.6066 | Acc: 79.000% (13746/17400) [============================>...................................................]  Step: 67ms | Tot: 11s906ms | Loss: 0.6072 | Acc: 78.978% (14058/17800) [=============================>..................................................]  Step: 68ms | Tot: 12s311ms | Loss: 0.6056 | Acc: 79.011% (14538/18400) [===============================>................................................]  Step: 66ms | Tot: 13s67ms | Loss: 0.6053 | Acc: 79.015% (15408/19500) [================================================>...............................]  Step: 67ms | Tot: 20s242ms | Loss: 0.6048 | Acc: 79.066% (23878/30200) [========================================================================>.......]  Step: 66ms | Tot: 30s558ms | Loss: 0.6000 | Acc: 79.228% (36128/45600) [=========================================================================>......]  Step: 68ms | Tot: 30s627ms | Loss: 0.6002 | Acc: 79.217% (36202/45700) [==========================================================================>.....]  Step: 66ms | Tot: 31s239ms | Loss: 0.6014 | Acc: 79.170% (36893/46600) [==============================================================================>.]  Step: 68ms | Tot: 32s751ms | Loss: 0.6039 | Acc: 79.086% (38673/48900)\n",
      "(302.00476559996605, 0.79084)\n",
      "\n",
      "===> epoch: 17/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 71ms | Tot: 33s961ms | Loss: 0.5866 | Acc: 79.862% (39931/50000)========>....................................................................]  Step: 67ms | Tot: 4s766ms | Loss: 0.5691 | Acc: 80.451% (5712/7100) [===========>....................................................................]  Step: 66ms | Tot: 4s967ms | Loss: 0.5726 | Acc: 80.351% (5946/7400) [============>...................................................................]  Step: 66ms | Tot: 5s169ms | Loss: 0.5807 | Acc: 80.091% (6167/7700) [=================>..............................................................]  Step: 65ms | Tot: 7s542ms | Loss: 0.5808 | Acc: 80.179% (8980/11200) [====================>...........................................................]  Step: 67ms | Tot: 8s647ms | Loss: 0.5845 | Acc: 80.039% (10245/12800) [========================>.......................................................]  Step: 66ms | Tot: 10s546ms | Loss: 0.5881 | Acc: 79.955% (12473/15600) [===========================================>....................................]  Step: 66ms | Tot: 18s508ms | Loss: 0.5861 | Acc: 79.810% (21868/27400) [==============================================>.................................]  Step: 67ms | Tot: 19s755ms | Loss: 0.5832 | Acc: 79.955% (23347/29200) [=================================================>..............................]  Step: 69ms | Tot: 20s902ms | Loss: 0.5849 | Acc: 79.883% (24684/30900) [==============================================================>.................]  Step: 66ms | Tot: 26s455ms | Loss: 0.5874 | Acc: 79.813% (31127/39000)\n",
      "(293.31957575678825, 0.79862)\n",
      "\n",
      "===> epoch: 18/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s615ms | Loss: 0.5637 | Acc: 80.620% (40310/50000)[================================================>...............................]  Step: 67ms | Tot: 20s443ms | Loss: 0.5609 | Acc: 80.781% (24719/30600) [====================================================>...........................]  Step: 67ms | Tot: 21s813ms | Loss: 0.5606 | Acc: 80.791% (26338/32600) [========================================================>.......................]  Step: 67ms | Tot: 23s714ms | Loss: 0.5633 | Acc: 80.718% (28574/35400) [================================================================>...............]  Step: 65ms | Tot: 27s161ms | Loss: 0.5634 | Acc: 80.664% (32669/40500) [================================================================>...............]  Step: 65ms | Tot: 27s227ms | Loss: 0.5632 | Acc: 80.675% (32754/40600) [==================================================================>.............]  Step: 67ms | Tot: 28s36ms | Loss: 0.5641 | Acc: 80.656% (33714/41800)\n",
      "(281.8609209507704, 0.8062)\n",
      "\n",
      "===> epoch: 19/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s709ms | Loss: 0.5479 | Acc: 81.210% (40605/50000)============>................................................................]  Step: 66ms | Tot: 6s515ms | Loss: 0.5154 | Acc: 82.194% (8055/9800) [====================>...........................................................]  Step: 66ms | Tot: 8s610ms | Loss: 0.5176 | Acc: 82.248% (10610/12900) [=============================>..................................................]  Step: 66ms | Tot: 12s293ms | Loss: 0.5225 | Acc: 81.918% (14991/18300) [=====================================>..........................................]  Step: 67ms | Tot: 15s917ms | Loss: 0.5357 | Acc: 81.523% (19321/23700) [==================================================>.............................]  Step: 67ms | Tot: 21s274ms | Loss: 0.5382 | Acc: 81.449% (25738/31600) [===========================================================>....................]  Step: 66ms | Tot: 24s868ms | Loss: 0.5422 | Acc: 81.385% (30031/36900) [============================================================>...................]  Step: 67ms | Tot: 25s410ms | Loss: 0.5426 | Acc: 81.347% (30668/37700) [============================================================>...................]  Step: 68ms | Tot: 25s676ms | Loss: 0.5427 | Acc: 81.341% (30991/38100) [=============================================================>..................]  Step: 66ms | Tot: 25s945ms | Loss: 0.5431 | Acc: 81.330% (31312/38500) [=======================================================================>........]  Step: 66ms | Tot: 29s907ms | Loss: 0.5452 | Acc: 81.313% (36103/44400) [===========================================================================>....]  Step: 67ms | Tot: 31s668ms | Loss: 0.5463 | Acc: 81.264% (38194/47000)\n",
      "(273.97160559892654, 0.8121)\n",
      "\n",
      "===> epoch: 20/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s438ms | Loss: 0.5261 | Acc: 81.598% (40799/50000)======>......................................................................]  Step: 67ms | Tot: 3s935ms | Loss: 0.5181 | Acc: 81.610% (4815/5900) [===============================================>................................]  Step: 67ms | Tot: 19s687ms | Loss: 0.5241 | Acc: 81.559% (24060/29500) [=======================================================>........................]  Step: 67ms | Tot: 23s137ms | Loss: 0.5221 | Acc: 81.633% (28245/34600) [======================================================================>.........]  Step: 68ms | Tot: 29s457ms | Loss: 0.5266 | Acc: 81.524% (35952/44100) [============================================================================>...]  Step: 67ms | Tot: 32s38ms | Loss: 0.5275 | Acc: 81.539% (39057/47900)\n",
      "(263.05205675959587, 0.81598)\n",
      "\n",
      "===> epoch: 21/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s466ms | Loss: 0.5073 | Acc: 82.596% (41298/50000)=====>.......................................................................]  Step: 67ms | Tot: 3s482ms | Loss: 0.4591 | Acc: 83.962% (4450/5300) [===========>....................................................................]  Step: 65ms | Tot: 4s650ms | Loss: 0.4663 | Acc: 83.929% (5875/7000) [======================================================================>.........]  Step: 67ms | Tot: 29s550ms | Loss: 0.5041 | Acc: 82.735% (36486/44100)\n",
      "(253.65782448649406, 0.82596)\n",
      "\n",
      "===> epoch: 22/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 68ms | Tot: 33s733ms | Loss: 0.4956 | Acc: 82.766% (41383/50000)==>..........................................................................]  Step: 67ms | Tot: 2s304ms | Loss: 0.4596 | Acc: 84.057% (2942/3500) [=====================>..........................................................]  Step: 67ms | Tot: 8s984ms | Loss: 0.4520 | Acc: 84.248% (11205/13300) [=====================>..........................................................]  Step: 67ms | Tot: 9s118ms | Loss: 0.4529 | Acc: 84.252% (11374/13500) [=======================>........................................................]  Step: 67ms | Tot: 9s931ms | Loss: 0.4569 | Acc: 84.190% (12376/14700) [===========================>....................................................]  Step: 64ms | Tot: 11s567ms | Loss: 0.4634 | Acc: 83.860% (14340/17100) [==============================>.................................................]  Step: 66ms | Tot: 13s9ms | Loss: 0.4678 | Acc: 83.698% (16070/19200) [======================================================>.........................]  Step: 66ms | Tot: 22s813ms | Loss: 0.4901 | Acc: 82.982% (28131/33900) [=======================================================>........................]  Step: 67ms | Tot: 23s219ms | Loss: 0.4903 | Acc: 82.994% (28633/34500) [=====================================================================>..........]  Step: 66ms | Tot: 29s389ms | Loss: 0.4969 | Acc: 82.716% (36064/43600) [=====================================================================>..........]  Step: 68ms | Tot: 29s457ms | Loss: 0.4969 | Acc: 82.709% (36144/43700)\n",
      "(247.80284544825554, 0.82766)\n",
      "\n",
      "===> epoch: 23/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 72ms | Tot: 33s546ms | Loss: 0.4825 | Acc: 83.344% (41672/50000)............................................................................]  Step: 64ms | Tot: 1s140ms | Loss: 0.4633 | Acc: 84.611% (1523/1800) [===================>............................................................]  Step: 66ms | Tot: 7s955ms | Loss: 0.4556 | Acc: 84.233% (10108/12000) [===================>............................................................]  Step: 66ms | Tot: 8s21ms | Loss: 0.4563 | Acc: 84.198% (10188/12100) [====================>...........................................................]  Step: 70ms | Tot: 8s555ms | Loss: 0.4534 | Acc: 84.202% (10862/12900) [====================>...........................................................]  Step: 65ms | Tot: 8s620ms | Loss: 0.4534 | Acc: 84.200% (10946/13000) [============================>...................................................]  Step: 66ms | Tot: 12s12ms | Loss: 0.4549 | Acc: 84.139% (15145/18000) [======================================>.........................................]  Step: 64ms | Tot: 15s821ms | Loss: 0.4572 | Acc: 84.143% (20026/23800) [===========================================>....................................]  Step: 66ms | Tot: 18s184ms | Loss: 0.4582 | Acc: 84.077% (22953/27300) [==================================================>.............................]  Step: 70ms | Tot: 20s926ms | Loss: 0.4670 | Acc: 83.812% (26317/31400) [==================================================================>.............]  Step: 67ms | Tot: 27s660ms | Loss: 0.4754 | Acc: 83.472% (34474/41300) [=========================================================================>......]  Step: 67ms | Tot: 30s653ms | Loss: 0.4811 | Acc: 83.350% (38091/45700)\n",
      "(241.23336631059647, 0.83344)\n",
      "\n",
      "===> epoch: 24/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 33s716ms | Loss: 0.4665 | Acc: 83.852% (41926/50000)>............................................................................]  Step: 67ms | Tot: 1s233ms | Loss: 0.4270 | Acc: 85.053% (1616/1900) [======>.........................................................................]  Step: 65ms | Tot: 2s797ms | Loss: 0.4490 | Acc: 84.643% (3555/4200) [===================>............................................................]  Step: 66ms | Tot: 8s192ms | Loss: 0.4334 | Acc: 84.943% (10363/12200) [======================>.........................................................]  Step: 67ms | Tot: 9s270ms | Loss: 0.4358 | Acc: 84.812% (11704/13800) [======================>.........................................................]  Step: 67ms | Tot: 9s471ms | Loss: 0.4380 | Acc: 84.801% (11957/14100) [=======================>........................................................]  Step: 65ms | Tot: 9s740ms | Loss: 0.4395 | Acc: 84.759% (12290/14500) [===============================>................................................]  Step: 67ms | Tot: 13s345ms | Loss: 0.4488 | Acc: 84.444% (16720/19800) [================================>...............................................]  Step: 67ms | Tot: 13s480ms | Loss: 0.4489 | Acc: 84.445% (16889/20000) [=========================================>......................................]  Step: 67ms | Tot: 17s325ms | Loss: 0.4511 | Acc: 84.416% (21695/25700) [===================================================>............................]  Step: 66ms | Tot: 21s862ms | Loss: 0.4622 | Acc: 83.972% (27207/32400) [==================================================================>.............]  Step: 65ms | Tot: 27s866ms | Loss: 0.4617 | Acc: 84.029% (34704/41300)\n",
      "(233.23234067857265, 0.83852)\n",
      "\n",
      "===> epoch: 25/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s754ms | Loss: 0.4558 | Acc: 84.410% (42205/50000)====>........................................................................]  Step: 67ms | Tot: 3s2ms | Loss: 0.4426 | Acc: 84.400% (3798/4500) [=======>........................................................................]  Step: 66ms | Tot: 3s137ms | Loss: 0.4434 | Acc: 84.340% (3964/4700) [=======>........................................................................]  Step: 67ms | Tot: 3s204ms | Loss: 0.4444 | Acc: 84.375% (4050/4800) [=======>........................................................................]  Step: 67ms | Tot: 3s272ms | Loss: 0.4434 | Acc: 84.408% (4136/4900) [==============>.................................................................]  Step: 68ms | Tot: 6s204ms | Loss: 0.4427 | Acc: 84.609% (7784/9200) [==========================>.....................................................]  Step: 67ms | Tot: 11s137ms | Loss: 0.4480 | Acc: 84.442% (13933/16500) [===========================>....................................................]  Step: 67ms | Tot: 11s687ms | Loss: 0.4489 | Acc: 84.382% (14598/17300) [=============================>..................................................]  Step: 69ms | Tot: 12s487ms | Loss: 0.4517 | Acc: 84.281% (15592/18500) [=====================================>..........................................]  Step: 67ms | Tot: 15s805ms | Loss: 0.4491 | Acc: 84.385% (19746/23400) [===========================================>....................................]  Step: 66ms | Tot: 18s364ms | Loss: 0.4541 | Acc: 84.214% (22822/27100) [===========================================>....................................]  Step: 67ms | Tot: 18s432ms | Loss: 0.4540 | Acc: 84.213% (22906/27200) [===========================================>....................................]  Step: 68ms | Tot: 18s500ms | Loss: 0.4542 | Acc: 84.198% (22986/27300) [============================================>...................................]  Step: 67ms | Tot: 18s770ms | Loss: 0.4551 | Acc: 84.173% (23316/27700) [===================================================================>............]  Step: 67ms | Tot: 28s248ms | Loss: 0.4567 | Acc: 84.301% (35322/41900) [===========================================================================>....]  Step: 68ms | Tot: 31s670ms | Loss: 0.4569 | Acc: 84.335% (39553/46900) [==============================================================================>.]  Step: 67ms | Tot: 33s31ms | Loss: 0.4572 | Acc: 84.358% (41251/48900) [==============================================================================>.]  Step: 65ms | Tot: 33s164ms | Loss: 0.4573 | Acc: 84.348% (41415/49100)\n",
      "(227.9112496227026, 0.8441)\n",
      "\n",
      "===> epoch: 26/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 69ms | Tot: 33s644ms | Loss: 0.4365 | Acc: 85.006% (42503/50000)===>.........................................................................]  Step: 67ms | Tot: 2s665ms | Loss: 0.3901 | Acc: 87.075% (3483/4000) [======>.........................................................................]  Step: 66ms | Tot: 2s801ms | Loss: 0.3892 | Acc: 87.167% (3661/4200) [=======>........................................................................]  Step: 66ms | Tot: 3s135ms | Loss: 0.3901 | Acc: 87.319% (4104/4700) [=========>......................................................................]  Step: 67ms | Tot: 3s872ms | Loss: 0.3913 | Acc: 87.276% (5062/5800) [===============>................................................................]  Step: 68ms | Tot: 6s310ms | Loss: 0.3978 | Acc: 86.798% (8159/9400) [============================>...................................................]  Step: 67ms | Tot: 12s34ms | Loss: 0.4181 | Acc: 85.721% (15344/17900) [============================================>...................................]  Step: 67ms | Tot: 18s616ms | Loss: 0.4279 | Acc: 85.324% (23720/27800) [===============================================>................................]  Step: 66ms | Tot: 19s765ms | Loss: 0.4298 | Acc: 85.278% (25157/29500) [============================================================>...................]  Step: 68ms | Tot: 25s565ms | Loss: 0.4343 | Acc: 85.108% (32426/38100) [======================================================================>.........]  Step: 66ms | Tot: 29s687ms | Loss: 0.4342 | Acc: 85.077% (37604/44200) [==============================================================================>.]  Step: 65ms | Tot: 32s884ms | Loss: 0.4350 | Acc: 85.057% (41593/48900)\n",
      "(218.24784217774868, 0.85006)\n",
      "\n",
      "===> epoch: 27/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s827ms | Loss: 0.4145 | Acc: 85.746% (42873/50000)...........................................................................]  Step: 66ms | Tot: 818ms | Loss: 0.3913 | Acc: 85.846% (1116/1300) [=========>......................................................................]  Step: 66ms | Tot: 3s808ms | Loss: 0.3895 | Acc: 86.351% (4922/5700) [=========>......................................................................]  Step: 65ms | Tot: 3s874ms | Loss: 0.3887 | Acc: 86.379% (5010/5800) [=========>......................................................................]  Step: 68ms | Tot: 4s9ms | Loss: 0.3891 | Acc: 86.450% (5187/6000) [=========>......................................................................]  Step: 67ms | Tot: 4s76ms | Loss: 0.3913 | Acc: 86.393% (5270/6100) [=================>..............................................................]  Step: 67ms | Tot: 7s467ms | Loss: 0.3955 | Acc: 86.586% (9611/11100) [==================>.............................................................]  Step: 67ms | Tot: 7s602ms | Loss: 0.3950 | Acc: 86.611% (9787/11300) [=======================>........................................................]  Step: 67ms | Tot: 10s33ms | Loss: 0.3977 | Acc: 86.523% (12892/14900) [============================>...................................................]  Step: 67ms | Tot: 12s240ms | Loss: 0.3995 | Acc: 86.552% (15666/18100) [===================================>............................................]  Step: 65ms | Tot: 15s175ms | Loss: 0.3978 | Acc: 86.429% (19360/22400) [======================================================>.........................]  Step: 66ms | Tot: 23s123ms | Loss: 0.4064 | Acc: 86.041% (29426/34200) [========================================================>.......................]  Step: 67ms | Tot: 23s888ms | Loss: 0.4072 | Acc: 86.008% (30361/35300) [============================================================>...................]  Step: 67ms | Tot: 25s731ms | Loss: 0.4059 | Acc: 86.013% (32685/38000) [=======================================================================>........]  Step: 65ms | Tot: 30s41ms | Loss: 0.4095 | Acc: 85.876% (38129/44400) [=======================================================================>........]  Step: 68ms | Tot: 30s373ms | Loss: 0.4098 | Acc: 85.857% (38550/44900) [=========================================================================>......]  Step: 66ms | Tot: 31s212ms | Loss: 0.4120 | Acc: 85.811% (39559/46100) [==========================================================================>.....]  Step: 68ms | Tot: 31s414ms | Loss: 0.4116 | Acc: 85.815% (39818/46400) [===============================================================================>]  Step: 67ms | Tot: 33s425ms | Loss: 0.4146 | Acc: 85.763% (42367/49400) [===============================================================================>]  Step: 67ms | Tot: 33s627ms | Loss: 0.4147 | Acc: 85.757% (42621/49700)\n",
      "(207.27447366714478, 0.85746)\n",
      "\n",
      "===> epoch: 28/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s754ms | Loss: 0.4125 | Acc: 85.824% (42912/50000)==>..........................................................................]  Step: 67ms | Tot: 2s468ms | Loss: 0.4076 | Acc: 85.378% (3159/3700) [==============>.................................................................]  Step: 67ms | Tot: 6s4ms | Loss: 0.3902 | Acc: 86.022% (7656/8900) [=======================>........................................................]  Step: 67ms | Tot: 9s811ms | Loss: 0.3989 | Acc: 86.048% (12477/14500) [=======================>........................................................]  Step: 67ms | Tot: 10s13ms | Loss: 0.4000 | Acc: 85.966% (12723/14800) [=============================>..................................................]  Step: 66ms | Tot: 12s362ms | Loss: 0.4012 | Acc: 85.967% (15732/18300) [==============================================>.................................]  Step: 66ms | Tot: 19s638ms | Loss: 0.4048 | Acc: 85.860% (25071/29200) [=========================================================>......................]  Step: 67ms | Tot: 24s19ms | Loss: 0.4067 | Acc: 85.891% (30663/35700) [=========================================================>......................]  Step: 67ms | Tot: 24s87ms | Loss: 0.4070 | Acc: 85.885% (30747/35800) [=========================================================>......................]  Step: 65ms | Tot: 24s356ms | Loss: 0.4065 | Acc: 85.895% (31094/36200) [=========================================================================>......]  Step: 66ms | Tot: 30s962ms | Loss: 0.4098 | Acc: 85.928% (39441/45900) [=========================================================================>......]  Step: 68ms | Tot: 31s30ms | Loss: 0.4098 | Acc: 85.924% (39525/46000) [==========================================================================>.....]  Step: 68ms | Tot: 31s298ms | Loss: 0.4099 | Acc: 85.914% (39864/46400) [============================================================================>...]  Step: 67ms | Tot: 32s325ms | Loss: 0.4107 | Acc: 85.873% (41133/47900) [===============================================================================>]  Step: 65ms | Tot: 33s415ms | Loss: 0.4119 | Acc: 85.834% (42488/49500)\n",
      "(206.27039566636086, 0.85824)\n",
      "\n",
      "===> epoch: 29/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 33s570ms | Loss: 0.4113 | Acc: 85.924% (42962/50000)=>...........................................................................]  Step: 67ms | Tot: 2s27ms | Loss: 0.3695 | Acc: 87.484% (2712/3100) [============>...................................................................]  Step: 67ms | Tot: 5s193ms | Loss: 0.3806 | Acc: 87.282% (6808/7800) [=============>..................................................................]  Step: 66ms | Tot: 5s663ms | Loss: 0.3801 | Acc: 87.188% (7411/8500) [==============>.................................................................]  Step: 69ms | Tot: 6s190ms | Loss: 0.3877 | Acc: 86.892% (8081/9300) [======================>.........................................................]  Step: 67ms | Tot: 9s262ms | Loss: 0.3938 | Acc: 86.449% (11930/13800) [=============================>..................................................]  Step: 66ms | Tot: 12s328ms | Loss: 0.3954 | Acc: 86.418% (15901/18400) [===============================>................................................]  Step: 67ms | Tot: 13s277ms | Loss: 0.3922 | Acc: 86.566% (17140/19800) [====================================>...........................................]  Step: 67ms | Tot: 15s522ms | Loss: 0.3981 | Acc: 86.394% (19957/23100) [=====================================>..........................................]  Step: 66ms | Tot: 15s589ms | Loss: 0.3991 | Acc: 86.349% (20033/23200) [=======================================>........................................]  Step: 66ms | Tot: 16s591ms | Loss: 0.4018 | Acc: 86.215% (21295/24700) [============================================================================>...]  Step: 65ms | Tot: 31s996ms | Loss: 0.4112 | Acc: 85.935% (40905/47600)\n",
      "(205.62735705077648, 0.85924)\n",
      "\n",
      "===> epoch: 30/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s806ms | Loss: 0.3937 | Acc: 86.536% (43268/50000)===========>.................................................................]  Step: 67ms | Tot: 5s941ms | Loss: 0.3542 | Acc: 88.044% (7924/9000) [==============>.................................................................]  Step: 67ms | Tot: 6s76ms | Loss: 0.3536 | Acc: 88.065% (8102/9200) [===============>................................................................]  Step: 66ms | Tot: 6s412ms | Loss: 0.3597 | Acc: 87.825% (8519/9700) [====================>...........................................................]  Step: 66ms | Tot: 8s703ms | Loss: 0.3627 | Acc: 87.725% (11492/13100) [==========================================>.....................................]  Step: 68ms | Tot: 17s961ms | Loss: 0.3813 | Acc: 86.985% (23051/26500) [=================================================>..............................]  Step: 66ms | Tot: 21s42ms | Loss: 0.3853 | Acc: 86.865% (26928/31000) [======================================================>.........................]  Step: 67ms | Tot: 23s233ms | Loss: 0.3881 | Acc: 86.764% (29760/34300) [====================================================================>...........]  Step: 66ms | Tot: 28s954ms | Loss: 0.3938 | Acc: 86.585% (36972/42700)\n",
      "(196.8683199286461, 0.86536)\n",
      "\n",
      "===> epoch: 31/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s123ms | Loss: 0.3873 | Acc: 86.718% (43359/50000)====>........................................................................]  Step: 66ms | Tot: 3s114ms | Loss: 0.3673 | Acc: 87.375% (4194/4800) [=========>......................................................................]  Step: 67ms | Tot: 3s780ms | Loss: 0.3732 | Acc: 87.052% (5049/5800) [=========>......................................................................]  Step: 66ms | Tot: 3s915ms | Loss: 0.3769 | Acc: 87.000% (5220/6000) [==============>.................................................................]  Step: 66ms | Tot: 5s903ms | Loss: 0.3742 | Acc: 87.089% (7838/9000) [==============>.................................................................]  Step: 66ms | Tot: 5s970ms | Loss: 0.3742 | Acc: 87.088% (7925/9100) [=======================>........................................................]  Step: 67ms | Tot: 9s748ms | Loss: 0.3692 | Acc: 87.306% (12834/14700) [==============================>.................................................]  Step: 66ms | Tot: 12s730ms | Loss: 0.3734 | Acc: 87.161% (16735/19200) [===================================================================>............]  Step: 67ms | Tot: 27s830ms | Loss: 0.3816 | Acc: 86.945% (36517/42000)\n",
      "(193.66562747955322, 0.86718)\n",
      "\n",
      "===> epoch: 32/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s159ms | Loss: 0.3684 | Acc: 87.300% (43650/50000)........................................................................]  Step: 67ms | Tot: 600ms | Loss: 0.3198 | Acc: 89.000% (890/1000) [=====>..........................................................................]  Step: 67ms | Tot: 2s205ms | Loss: 0.3405 | Acc: 88.559% (3011/3400) [===========>....................................................................]  Step: 67ms | Tot: 4s661ms | Loss: 0.3406 | Acc: 88.380% (6275/7100) [==========================>.....................................................]  Step: 66ms | Tot: 10s757ms | Loss: 0.3511 | Acc: 87.945% (14335/16300) [==============================>.................................................]  Step: 66ms | Tot: 12s418ms | Loss: 0.3522 | Acc: 87.851% (16516/18800) [==============================>.................................................]  Step: 67ms | Tot: 12s552ms | Loss: 0.3542 | Acc: 87.800% (16682/19000) [===================================>............................................]  Step: 66ms | Tot: 14s668ms | Loss: 0.3565 | Acc: 87.685% (19466/22200) [============================================>...................................]  Step: 66ms | Tot: 18s257ms | Loss: 0.3541 | Acc: 87.761% (24222/27600) [============================================>...................................]  Step: 66ms | Tot: 18s324ms | Loss: 0.3544 | Acc: 87.758% (24309/27700) [======================================================>.........................]  Step: 67ms | Tot: 22s654ms | Loss: 0.3610 | Acc: 87.564% (29947/34200) [==========================================================>.....................]  Step: 66ms | Tot: 24s54ms | Loss: 0.3623 | Acc: 87.532% (31774/36300) [===========================================================>....................]  Step: 67ms | Tot: 24s656ms | Loss: 0.3623 | Acc: 87.527% (32560/37200) [============================================================>...................]  Step: 66ms | Tot: 24s857ms | Loss: 0.3623 | Acc: 87.528% (32823/37500) [===================================================================>............]  Step: 67ms | Tot: 28s139ms | Loss: 0.3647 | Acc: 87.429% (37070/42400) [====================================================================>...........]  Step: 66ms | Tot: 28s272ms | Loss: 0.3651 | Acc: 87.404% (37234/42600)\n",
      "(184.21495062112808, 0.873)\n",
      "\n",
      "===> epoch: 33/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s209ms | Loss: 0.3621 | Acc: 87.588% (43794/50000).......................................................................]  Step: 66ms | Tot: 132ms | Loss: 0.3909 | Acc: 87.667% (263/300) [====>...........................................................................]  Step: 68ms | Tot: 1s724ms | Loss: 0.3385 | Acc: 87.963% (2375/2700) [========>.......................................................................]  Step: 66ms | Tot: 3s457ms | Loss: 0.3504 | Acc: 87.887% (4658/5300) [==============>.................................................................]  Step: 66ms | Tot: 5s894ms | Loss: 0.3496 | Acc: 87.989% (7831/8900) [===============>................................................................]  Step: 67ms | Tot: 6s563ms | Loss: 0.3464 | Acc: 88.091% (8721/9900) [===================================================================>............]  Step: 67ms | Tot: 27s926ms | Loss: 0.3609 | Acc: 87.682% (36914/42100) [=====================================================================>..........]  Step: 66ms | Tot: 28s950ms | Loss: 0.3597 | Acc: 87.718% (38245/43600) [=======================================================================>........]  Step: 66ms | Tot: 29s618ms | Loss: 0.3591 | Acc: 87.747% (39135/44600) [=========================================================================>......]  Step: 66ms | Tot: 30s637ms | Loss: 0.3599 | Acc: 87.711% (40435/46100)\n",
      "(181.06263533234596, 0.87588)\n",
      "\n",
      "===> epoch: 34/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s961ms | Loss: 0.3513 | Acc: 87.742% (43871/50000).....................................................................]  Step: 66ms | Tot: 66ms | Loss: 0.4416 | Acc: 83.500% (167/200) [>...............................................................................]  Step: 66ms | Tot: 267ms | Loss: 0.3632 | Acc: 87.800% (439/500) [===============================================>................................]  Step: 69ms | Tot: 19s456ms | Loss: 0.3430 | Acc: 88.041% (26060/29600) [================================================>...............................]  Step: 67ms | Tot: 19s790ms | Loss: 0.3429 | Acc: 88.023% (26495/30100)\n",
      "(175.66649308800697, 0.87742)\n",
      "\n",
      "===> epoch: 35/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s230ms | Loss: 0.3476 | Acc: 88.130% (44065/50000)====================>...........................................................]  Step: 69ms | Tot: 8s683ms | Loss: 0.3301 | Acc: 88.846% (11550/13000) [=====================================================================>..........]  Step: 67ms | Tot: 27s975ms | Loss: 0.3445 | Acc: 88.300% (38234/43300)\n",
      "(173.80864901840687, 0.8813)\n",
      "\n",
      "===> epoch: 36/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 32s549ms | Loss: 0.3395 | Acc: 88.500% (44250/50000)[=============================================>..................................]  Step: 67ms | Tot: 18s411ms | Loss: 0.3261 | Acc: 89.028% (25195/28300) [=============================================>..................................]  Step: 66ms | Tot: 18s545ms | Loss: 0.3264 | Acc: 89.028% (25373/28500) [===================================================>............................]  Step: 66ms | Tot: 20s777ms | Loss: 0.3257 | Acc: 89.044% (28405/31900) [================================================================>...............]  Step: 66ms | Tot: 26s510ms | Loss: 0.3360 | Acc: 88.638% (35987/40600) [=================================================================>..............]  Step: 66ms | Tot: 26s930ms | Loss: 0.3366 | Acc: 88.607% (36506/41200)\n",
      "(169.74347871541977, 0.885)\n",
      "\n",
      "===> epoch: 37/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s935ms | Loss: 0.3371 | Acc: 88.554% (44277/50000)[=============================================>..................................]  Step: 66ms | Tot: 18s331ms | Loss: 0.3251 | Acc: 88.892% (25512/28700) [==============================================>.................................]  Step: 67ms | Tot: 18s399ms | Loss: 0.3252 | Acc: 88.889% (25600/28800) [==============================================>.................................]  Step: 66ms | Tot: 18s466ms | Loss: 0.3249 | Acc: 88.900% (25692/28900) [================================================>...............................]  Step: 67ms | Tot: 19s463ms | Loss: 0.3265 | Acc: 88.855% (27012/30400)\n",
      "(168.55231115221977, 0.88554)\n",
      "\n",
      "===> epoch: 38/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s989ms | Loss: 0.3208 | Acc: 89.120% (44560/50000)[======================================>.........................................]  Step: 67ms | Tot: 15s451ms | Loss: 0.3170 | Acc: 89.300% (21700/24300) [================================================>...............................]  Step: 67ms | Tot: 19s412ms | Loss: 0.3186 | Acc: 89.247% (27131/30400) [======================================================================>.........]  Step: 66ms | Tot: 28s47ms | Loss: 0.3204 | Acc: 89.180% (39150/43900) [======================================================================>.........]  Step: 66ms | Tot: 28s114ms | Loss: 0.3204 | Acc: 89.177% (39238/44000) [=======================================================================>........]  Step: 67ms | Tot: 28s708ms | Loss: 0.3212 | Acc: 89.138% (40023/44900)\n",
      "(160.3811987489462, 0.8912)\n",
      "\n",
      "===> epoch: 39/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s770ms | Loss: 0.3051 | Acc: 89.550% (44775/50000)[==============================>.................................................]  Step: 67ms | Tot: 12s225ms | Loss: 0.2846 | Acc: 90.255% (17329/19200)\n",
      "(152.54114601761103, 0.8955)\n",
      "\n",
      "===> epoch: 40/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s767ms | Loss: 0.3056 | Acc: 89.604% (44802/50000)[========================================================>.......................]  Step: 66ms | Tot: 22s261ms | Loss: 0.2970 | Acc: 89.820% (31437/35000) [===========================================================>....................]  Step: 65ms | Tot: 23s822ms | Loss: 0.2985 | Acc: 89.749% (33566/37400)\n",
      "(152.82469902932644, 0.89604)\n",
      "\n",
      "===> epoch: 41/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s888ms | Loss: 0.2930 | Acc: 89.926% (44963/50000)[========================================>.......................................]  Step: 67ms | Tot: 16s220ms | Loss: 0.2847 | Acc: 90.201% (22911/25400) [========================================>.......................................]  Step: 66ms | Tot: 16s287ms | Loss: 0.2847 | Acc: 90.196% (23000/25500) [========================================>.......................................]  Step: 66ms | Tot: 16s353ms | Loss: 0.2843 | Acc: 90.211% (23094/25600) [====================================================>...........................]  Step: 68ms | Tot: 20s769ms | Loss: 0.2879 | Acc: 90.092% (29280/32500) [=======================================================================>........]  Step: 64ms | Tot: 28s351ms | Loss: 0.2914 | Acc: 90.002% (39961/44400) [=======================================================================>........]  Step: 63ms | Tot: 28s414ms | Loss: 0.2914 | Acc: 89.998% (40049/44500)\n",
      "(146.48400340229273, 0.89926)\n",
      "\n",
      "===> epoch: 42/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s850ms | Loss: 0.2922 | Acc: 90.168% (45084/50000)=========>...................................................................]  Step: 67ms | Tot: 4s732ms | Loss: 0.2605 | Acc: 91.307% (6848/7500) [============>...................................................................]  Step: 65ms | Tot: 4s932ms | Loss: 0.2616 | Acc: 91.295% (7121/7800) [========================================>.......................................]  Step: 66ms | Tot: 16s251ms | Loss: 0.2720 | Acc: 90.815% (23067/25400)\n",
      "(146.12486294656992, 0.90168)\n",
      "\n",
      "===> epoch: 43/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s716ms | Loss: 0.2947 | Acc: 89.806% (44903/50000)[=========================================================>......................]  Step: 62ms | Tot: 22s769ms | Loss: 0.2905 | Acc: 89.967% (32478/36100) [==============================================================>.................]  Step: 66ms | Tot: 24s876ms | Loss: 0.2930 | Acc: 89.888% (35326/39300)\n",
      "(147.33084535598755, 0.89806)\n",
      "\n",
      "===> epoch: 44/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 32s109ms | Loss: 0.2691 | Acc: 90.838% (45419/50000)===>.........................................................................]  Step: 66ms | Tot: 2s615ms | Loss: 0.2505 | Acc: 90.810% (3814/4200)\n",
      "(134.53514900058508, 0.90838)\n",
      "\n",
      "===> epoch: 45/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s63ms | Loss: 0.2767 | Acc: 90.680% (45340/50000) [==================================>.............................................]  Step: 66ms | Tot: 13s661ms | Loss: 0.2548 | Acc: 91.249% (19436/21300) [==============================================>.................................]  Step: 67ms | Tot: 18s745ms | Loss: 0.2602 | Acc: 91.120% (26607/29200)\n",
      "(138.32825769484043, 0.9068)\n",
      "\n",
      "===> epoch: 46/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s847ms | Loss: 0.2651 | Acc: 91.000% (45500/50000)====================>...........................................................]  Step: 66ms | Tot: 8s368ms | Loss: 0.2423 | Acc: 91.924% (12042/13100)\n",
      "(132.5260756984353, 0.91)\n",
      "\n",
      "===> epoch: 47/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 32s296ms | Loss: 0.2609 | Acc: 91.210% (45605/50000)========================>.......................................................]  Step: 67ms | Tot: 9s961ms | Loss: 0.2499 | Acc: 91.381% (14164/15500) [=================================>..............................................]  Step: 64ms | Tot: 13s574ms | Loss: 0.2500 | Acc: 91.429% (19200/21000)\n",
      "(130.47422294318676, 0.9121)\n",
      "\n",
      "===> epoch: 48/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s936ms | Loss: 0.2570 | Acc: 91.264% (45632/50000)==================>.............................................................]  Step: 66ms | Tot: 7s430ms | Loss: 0.2427 | Acc: 91.548% (10528/11500)\n",
      "(128.4819196909666, 0.91264)\n",
      "\n",
      "===> epoch: 49/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s801ms | Loss: 0.2474 | Acc: 91.622% (45811/50000)[=====================================================================>..........]  Step: 66ms | Tot: 27s468ms | Loss: 0.2407 | Acc: 91.819% (39666/43200)\n",
      "(123.691970102489, 0.91622)\n",
      "\n",
      "===> epoch: 50/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s966ms | Loss: 0.2520 | Acc: 91.700% (45850/50000)\n",
      "(126.01365967094898, 0.917)\n",
      "\n",
      "===> epoch: 51/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 32s51ms | Loss: 0.2381 | Acc: 91.984% (45992/50000) [=====================================>..........................................]  Step: 66ms | Tot: 14s970ms | Loss: 0.2293 | Acc: 92.226% (21581/23400) [==============================================>.................................]  Step: 66ms | Tot: 18s553ms | Loss: 0.2302 | Acc: 92.269% (26758/29000)\n",
      "(119.04647308588028, 0.91984)\n",
      "\n",
      "===> epoch: 52/100\n",
      "train:\n",
      " 500/500 [===============================================================================>]  Step: 67ms | Tot: 31s682ms | Loss: 0.2430 | Acc: 91.912% (45772/49800) [============================================================================>...]  Step: 66ms | Tot: 30s486ms | Loss: 0.2427 | Acc: 91.921% (44122/48000) [==============================================================================>.]  Step: 66ms | Tot: 31s84ms | Loss: 0.2428 | Acc: 91.926% (44952/48900) [===============================================================================>]  Step: 66ms | Tot: 31s748ms | Loss: 0.2429 | Acc: 91.910% (45863/49900) [================================================================================>]  Step: 66ms | Tot: 31s815ms | Loss: 0.2428 | Acc: 91.914% (45957/50000)\n",
      "(121.38545370101929, 0.91914)\n",
      "\n",
      "===> epoch: 53/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s969ms | Loss: 0.2249 | Acc: 92.432% (46216/50000)[===========================>....................................................]  Step: 67ms | Tot: 10s611ms | Loss: 0.2134 | Acc: 92.864% (15694/16900) [================================>...............................................]  Step: 66ms | Tot: 12s709ms | Loss: 0.2180 | Acc: 92.662% (18625/20100) [=========================================>......................................]  Step: 66ms | Tot: 16s647ms | Loss: 0.2188 | Acc: 92.618% (24266/26200)\n",
      "(112.47385290265083, 0.92432)\n",
      "\n",
      "===> epoch: 54/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 32s432ms | Loss: 0.2307 | Acc: 92.364% (46182/50000)=======================>........................................................]  Step: 66ms | Tot: 9s444ms | Loss: 0.2042 | Acc: 93.245% (13707/14700) [=======================>........................................................]  Step: 67ms | Tot: 9s511ms | Loss: 0.2043 | Acc: 93.243% (13800/14800) [========================>.......................................................]  Step: 66ms | Tot: 9s780ms | Loss: 0.2060 | Acc: 93.197% (14166/15200) [=========================>......................................................]  Step: 65ms | Tot: 10s446ms | Loss: 0.2110 | Acc: 93.037% (15072/16200)\n",
      "(115.36477098613977, 0.92364)\n",
      "\n",
      "===> epoch: 55/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 62ms | Tot: 31s962ms | Loss: 0.2179 | Acc: 92.738% (46369/50000)=>...........................................................................]  Step: 67ms | Tot: 1s601ms | Loss: 0.2253 | Acc: 92.560% (2314/2500) [====>...........................................................................]  Step: 66ms | Tot: 1s734ms | Loss: 0.2256 | Acc: 92.556% (2499/2700) [====>...........................................................................]  Step: 66ms | Tot: 1s935ms | Loss: 0.2330 | Acc: 92.300% (2769/3000) [===================================>............................................]  Step: 66ms | Tot: 14s107ms | Loss: 0.2171 | Acc: 92.804% (20324/21900)\n",
      "(108.96799312531948, 0.92738)\n",
      "\n",
      "===> epoch: 56/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 31s930ms | Loss: 0.2182 | Acc: 92.848% (46424/50000)[=======================================================================>........]  Step: 67ms | Tot: 28s428ms | Loss: 0.2182 | Acc: 92.892% (41430/44600) [===============================================================================>]  Step: 66ms | Tot: 31s601ms | Loss: 0.2182 | Acc: 92.840% (45956/49500)\n",
      "(109.09385034441948, 0.92848)\n",
      "\n",
      "===> epoch: 57/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 63ms | Tot: 31s619ms | Loss: 0.2043 | Acc: 93.308% (46654/50000)===========>.................................................................]  Step: 65ms | Tot: 5s492ms | Loss: 0.2070 | Acc: 93.227% (8204/8800) [==============================================>.................................]  Step: 70ms | Tot: 18s518ms | Loss: 0.2044 | Acc: 93.287% (27333/29300) [===============================================>................................]  Step: 67ms | Tot: 18s855ms | Loss: 0.2037 | Acc: 93.332% (27813/29800)\n",
      "(102.17180896177888, 0.93308)\n",
      "\n",
      "===> epoch: 58/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 32s912ms | Loss: 0.2229 | Acc: 92.662% (46331/50000)[========================================>.......................................]  Step: 66ms | Tot: 16s547ms | Loss: 0.2084 | Acc: 93.188% (23763/25500) [========================================>.......................................]  Step: 67ms | Tot: 16s614ms | Loss: 0.2085 | Acc: 93.199% (23859/25600) [=========================================>......................................]  Step: 66ms | Tot: 16s882ms | Loss: 0.2092 | Acc: 93.162% (24222/26000) [=============================================>..................................]  Step: 67ms | Tot: 18s356ms | Loss: 0.2085 | Acc: 93.188% (26279/28200) [==================================================>.............................]  Step: 67ms | Tot: 20s749ms | Loss: 0.2083 | Acc: 93.201% (29638/31800) [===================================================>............................]  Step: 66ms | Tot: 21s149ms | Loss: 0.2095 | Acc: 93.145% (30179/32400) [=======================================================>........................]  Step: 66ms | Tot: 22s489ms | Loss: 0.2128 | Acc: 93.032% (32003/34400) [=================================================================>..............]  Step: 66ms | Tot: 26s911ms | Loss: 0.2166 | Acc: 92.878% (38080/41000) [===================================================================>............]  Step: 66ms | Tot: 27s646ms | Loss: 0.2186 | Acc: 92.819% (39077/42100) [===================================================================>............]  Step: 66ms | Tot: 27s713ms | Loss: 0.2187 | Acc: 92.810% (39166/42200) [====================================================================>...........]  Step: 67ms | Tot: 27s914ms | Loss: 0.2187 | Acc: 92.809% (39444/42500) [==========================================================================>.....]  Step: 66ms | Tot: 30s713ms | Loss: 0.2223 | Acc: 92.690% (43286/46700) [===========================================================================>....]  Step: 66ms | Tot: 30s848ms | Loss: 0.2226 | Acc: 92.676% (43465/46900) [===========================================================================>....]  Step: 67ms | Tot: 31s182ms | Loss: 0.2222 | Acc: 92.679% (43930/47400) [============================================================================>...]  Step: 66ms | Tot: 31s316ms | Loss: 0.2223 | Acc: 92.676% (44114/47600)\n",
      "(111.46101229637861, 0.92662)\n",
      "\n",
      "===> epoch: 59/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s484ms | Loss: 0.2078 | Acc: 93.162% (46581/50000)........................................................................]  Step: 66ms | Tot: 531ms | Loss: 0.1562 | Acc: 94.889% (854/900) [===>............................................................................]  Step: 67ms | Tot: 1s398ms | Loss: 0.1607 | Acc: 94.909% (2088/2200) [=======>........................................................................]  Step: 66ms | Tot: 3s | Loss: 0.1664 | Acc: 94.761% (4359/4600) [=========>......................................................................]  Step: 66ms | Tot: 3s932ms | Loss: 0.1682 | Acc: 94.567% (5674/6000) [==========>.....................................................................]  Step: 67ms | Tot: 4s199ms | Loss: 0.1702 | Acc: 94.531% (6050/6400) [============>...................................................................]  Step: 67ms | Tot: 5s1ms | Loss: 0.1705 | Acc: 94.434% (7177/7600) [==============>.................................................................]  Step: 66ms | Tot: 5s868ms | Loss: 0.1767 | Acc: 94.213% (8385/8900) [====================>...........................................................]  Step: 66ms | Tot: 8s472ms | Loss: 0.1880 | Acc: 94.008% (11939/12700) [=====================>..........................................................]  Step: 66ms | Tot: 9s144ms | Loss: 0.1895 | Acc: 93.927% (12868/13700) [======================>.........................................................]  Step: 66ms | Tot: 9s480ms | Loss: 0.1884 | Acc: 93.937% (13339/14200) [==============================>.................................................]  Step: 66ms | Tot: 12s592ms | Loss: 0.1868 | Acc: 93.936% (17660/18800) [==============================>.................................................]  Step: 66ms | Tot: 12s726ms | Loss: 0.1866 | Acc: 93.947% (17850/19000) [===============================>................................................]  Step: 66ms | Tot: 13s58ms | Loss: 0.1860 | Acc: 93.995% (18329/19500) [=====================================>..........................................]  Step: 66ms | Tot: 15s593ms | Loss: 0.1876 | Acc: 93.961% (21799/23200) [======================================>.........................................]  Step: 66ms | Tot: 16s330ms | Loss: 0.1905 | Acc: 93.897% (22817/24300) [========================================>.......................................]  Step: 67ms | Tot: 16s932ms | Loss: 0.1922 | Acc: 93.837% (23647/25200) [========================================>.......................................]  Step: 66ms | Tot: 16s999ms | Loss: 0.1924 | Acc: 93.838% (23741/25300) [===========================================>....................................]  Step: 67ms | Tot: 18s208ms | Loss: 0.1959 | Acc: 93.690% (25390/27100) [===========================================>....................................]  Step: 67ms | Tot: 18s409ms | Loss: 0.1956 | Acc: 93.701% (25674/27400) [============================================>...................................]  Step: 67ms | Tot: 18s477ms | Loss: 0.1958 | Acc: 93.684% (25763/27500) [=============================================>..................................]  Step: 67ms | Tot: 19s14ms | Loss: 0.1959 | Acc: 93.671% (26509/28300) [=============================================>..................................]  Step: 66ms | Tot: 19s214ms | Loss: 0.1959 | Acc: 93.661% (26787/28600) [==============================================>.................................]  Step: 66ms | Tot: 19s616ms | Loss: 0.1962 | Acc: 93.664% (27350/29200) [===============================================>................................]  Step: 66ms | Tot: 19s750ms | Loss: 0.1968 | Acc: 93.650% (27533/29400) [================================================>...............................]  Step: 66ms | Tot: 20s350ms | Loss: 0.1966 | Acc: 93.640% (28373/30300) [===================================================>............................]  Step: 66ms | Tot: 21s552ms | Loss: 0.1965 | Acc: 93.592% (30043/32100) [=======================================================>........................]  Step: 66ms | Tot: 23s151ms | Loss: 0.1980 | Acc: 93.554% (32276/34500) [=============================================================>..................]  Step: 66ms | Tot: 25s620ms | Loss: 0.2039 | Acc: 93.361% (35664/38200) [===============================================================>................]  Step: 66ms | Tot: 26s620ms | Loss: 0.2054 | Acc: 93.310% (37044/39700) [===============================================================>................]  Step: 66ms | Tot: 26s686ms | Loss: 0.2057 | Acc: 93.296% (37132/39800) [==================================================================>.............]  Step: 66ms | Tot: 27s818ms | Loss: 0.2048 | Acc: 93.294% (38717/41500) [============================================================================>...]  Step: 67ms | Tot: 31s816ms | Loss: 0.2072 | Acc: 93.179% (44260/47500)\n",
      "(103.92419639602304, 0.93162)\n",
      "\n",
      "===> epoch: 60/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s382ms | Loss: 0.2140 | Acc: 92.808% (46404/50000)=>...........................................................................]  Step: 66ms | Tot: 1s862ms | Loss: 0.1784 | Acc: 93.690% (2717/2900) [=========>......................................................................]  Step: 66ms | Tot: 3s856ms | Loss: 0.1766 | Acc: 93.746% (5531/5900) [=========>......................................................................]  Step: 67ms | Tot: 4s57ms | Loss: 0.1758 | Acc: 93.758% (5813/6200) [==========>.....................................................................]  Step: 67ms | Tot: 4s258ms | Loss: 0.1751 | Acc: 93.785% (6096/6500) [==========>.....................................................................]  Step: 66ms | Tot: 4s391ms | Loss: 0.1759 | Acc: 93.776% (6283/6700) [=================>..............................................................]  Step: 68ms | Tot: 7s324ms | Loss: 0.1860 | Acc: 93.559% (10385/11100) [==================>.............................................................]  Step: 66ms | Tot: 7s524ms | Loss: 0.1880 | Acc: 93.509% (10660/11400) [===================>............................................................]  Step: 67ms | Tot: 7s926ms | Loss: 0.1930 | Acc: 93.325% (11199/12000) [======================>.........................................................]  Step: 68ms | Tot: 9s326ms | Loss: 0.2078 | Acc: 92.837% (13090/14100) [========================>.......................................................]  Step: 67ms | Tot: 10s128ms | Loss: 0.2116 | Acc: 92.752% (14191/15300) [============================>...................................................]  Step: 66ms | Tot: 11s596ms | Loss: 0.2073 | Acc: 92.857% (16250/17500) [============================>...................................................]  Step: 66ms | Tot: 11s862ms | Loss: 0.2075 | Acc: 92.866% (16623/17900) [============================>...................................................]  Step: 67ms | Tot: 11s930ms | Loss: 0.2077 | Acc: 92.867% (16716/18000) [============================>...................................................]  Step: 67ms | Tot: 11s997ms | Loss: 0.2073 | Acc: 92.878% (16811/18100) [=============================>..................................................]  Step: 66ms | Tot: 12s330ms | Loss: 0.2070 | Acc: 92.898% (17279/18600) [===============================>................................................]  Step: 67ms | Tot: 13s1ms | Loss: 0.2060 | Acc: 92.878% (18204/19600) [================================>...............................................]  Step: 66ms | Tot: 13s536ms | Loss: 0.2062 | Acc: 92.882% (18948/20400) [================================>...............................................]  Step: 66ms | Tot: 13s603ms | Loss: 0.2063 | Acc: 92.868% (19038/20500) [===================================>............................................]  Step: 66ms | Tot: 14s878ms | Loss: 0.2077 | Acc: 92.888% (20807/22400) [====================================>...........................................]  Step: 67ms | Tot: 15s348ms | Loss: 0.2097 | Acc: 92.835% (21445/23100) [=====================================>..........................................]  Step: 66ms | Tot: 15s549ms | Loss: 0.2101 | Acc: 92.808% (21717/23400) [=====================================>..........................................]  Step: 66ms | Tot: 15s616ms | Loss: 0.2101 | Acc: 92.804% (21809/23500) [=======================================>........................................]  Step: 67ms | Tot: 16s218ms | Loss: 0.2124 | Acc: 92.754% (22632/24400) [=======================================>........................................]  Step: 66ms | Tot: 16s488ms | Loss: 0.2126 | Acc: 92.734% (22998/24800) [===========================================>....................................]  Step: 66ms | Tot: 17s887ms | Loss: 0.2141 | Acc: 92.688% (24933/26900) [===========================================>....................................]  Step: 66ms | Tot: 17s954ms | Loss: 0.2144 | Acc: 92.663% (25019/27000) [===========================================>....................................]  Step: 67ms | Tot: 18s22ms | Loss: 0.2146 | Acc: 92.649% (25108/27100) [============================================>...................................]  Step: 66ms | Tot: 18s624ms | Loss: 0.2152 | Acc: 92.643% (25940/28000) [================================================>...............................]  Step: 66ms | Tot: 20s41ms | Loss: 0.2143 | Acc: 92.718% (27908/30100) [==================================================>.............................]  Step: 67ms | Tot: 20s906ms | Loss: 0.2123 | Acc: 92.777% (29132/31400) [===================================================>............................]  Step: 67ms | Tot: 21s375ms | Loss: 0.2116 | Acc: 92.813% (29793/32100) [===================================================>............................]  Step: 66ms | Tot: 21s442ms | Loss: 0.2116 | Acc: 92.811% (29885/32200) [====================================================>...........................]  Step: 66ms | Tot: 21s709ms | Loss: 0.2120 | Acc: 92.810% (30256/32600) [====================================================>...........................]  Step: 67ms | Tot: 21s777ms | Loss: 0.2121 | Acc: 92.801% (30346/32700) [=====================================================>..........................]  Step: 67ms | Tot: 22s247ms | Loss: 0.2132 | Acc: 92.781% (30989/33400) [=====================================================>..........................]  Step: 66ms | Tot: 22s380ms | Loss: 0.2130 | Acc: 92.792% (31178/33600) [======================================================>.........................]  Step: 67ms | Tot: 22s582ms | Loss: 0.2131 | Acc: 92.799% (31459/33900) [========================================================>.......................]  Step: 67ms | Tot: 23s385ms | Loss: 0.2149 | Acc: 92.752% (32556/35100) [============================================================>...................]  Step: 66ms | Tot: 25s251ms | Loss: 0.2151 | Acc: 92.747% (35151/37900) [=============================================================>..................]  Step: 67ms | Tot: 25s452ms | Loss: 0.2151 | Acc: 92.754% (35432/38200) [==============================================================>.................]  Step: 67ms | Tot: 25s920ms | Loss: 0.2148 | Acc: 92.756% (36082/38900) [================================================================>...............]  Step: 67ms | Tot: 26s719ms | Loss: 0.2148 | Acc: 92.736% (37187/40100) [================================================================>...............]  Step: 67ms | Tot: 26s786ms | Loss: 0.2146 | Acc: 92.744% (37283/40200) [=================================================================>..............]  Step: 67ms | Tot: 27s121ms | Loss: 0.2150 | Acc: 92.742% (37746/40700) [====================================================================>...........]  Step: 67ms | Tot: 28s322ms | Loss: 0.2154 | Acc: 92.739% (39414/42500) [====================================================================>...........]  Step: 66ms | Tot: 28s389ms | Loss: 0.2155 | Acc: 92.735% (39505/42600) [====================================================================>...........]  Step: 67ms | Tot: 28s656ms | Loss: 0.2157 | Acc: 92.721% (39870/43000) [==============================================================================>.]  Step: 66ms | Tot: 32s575ms | Loss: 0.2144 | Acc: 92.795% (45284/48800)\n",
      "(106.99460561200976, 0.92808)\n",
      "\n",
      "===> epoch: 61/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s553ms | Loss: 0.2030 | Acc: 93.240% (46620/50000)............................................................................]  Step: 68ms | Tot: 1s136ms | Loss: 0.1859 | Acc: 93.778% (1688/1800) [===>............................................................................]  Step: 67ms | Tot: 1s536ms | Loss: 0.1796 | Acc: 93.875% (2253/2400) [==============>.................................................................]  Step: 67ms | Tot: 5s993ms | Loss: 0.1642 | Acc: 94.478% (8503/9000) [===============>................................................................]  Step: 66ms | Tot: 6s529ms | Loss: 0.1644 | Acc: 94.602% (9271/9800) [============================>...................................................]  Step: 67ms | Tot: 11s956ms | Loss: 0.1718 | Acc: 94.348% (16794/17800) [============================>...................................................]  Step: 67ms | Tot: 12s90ms | Loss: 0.1716 | Acc: 94.344% (16982/18000) [=============================>..................................................]  Step: 67ms | Tot: 12s423ms | Loss: 0.1731 | Acc: 94.297% (17445/18500) [===============================>................................................]  Step: 66ms | Tot: 13s226ms | Loss: 0.1752 | Acc: 94.218% (18561/19700) [===============================>................................................]  Step: 66ms | Tot: 13s293ms | Loss: 0.1755 | Acc: 94.207% (18653/19800) [================================>...............................................]  Step: 67ms | Tot: 13s560ms | Loss: 0.1763 | Acc: 94.158% (19020/20200) [=================================>..............................................]  Step: 67ms | Tot: 13s961ms | Loss: 0.1777 | Acc: 94.106% (19574/20800) [====================================>...........................................]  Step: 66ms | Tot: 15s355ms | Loss: 0.1809 | Acc: 93.987% (21523/22900) [=====================================>..........................................]  Step: 67ms | Tot: 15s623ms | Loss: 0.1828 | Acc: 93.974% (21896/23300) [============================================>...................................]  Step: 66ms | Tot: 18s606ms | Loss: 0.1887 | Acc: 93.773% (25975/27700) [=============================================>..................................]  Step: 67ms | Tot: 19s141ms | Loss: 0.1882 | Acc: 93.800% (26733/28500) [==============================================>.................................]  Step: 67ms | Tot: 19s543ms | Loss: 0.1883 | Acc: 93.790% (27293/29100) [================================================>...............................]  Step: 67ms | Tot: 20s285ms | Loss: 0.1891 | Acc: 93.752% (28313/30200) [================================================>...............................]  Step: 67ms | Tot: 20s352ms | Loss: 0.1887 | Acc: 93.769% (28412/30300) [=================================================>..............................]  Step: 67ms | Tot: 20s886ms | Loss: 0.1895 | Acc: 93.749% (29156/31100) [=================================================>..............................]  Step: 66ms | Tot: 20s953ms | Loss: 0.1895 | Acc: 93.740% (29247/31200) [=====================================================>..........................]  Step: 66ms | Tot: 22s617ms | Loss: 0.1913 | Acc: 93.697% (31576/33700) [=======================================================>........................]  Step: 66ms | Tot: 23s150ms | Loss: 0.1940 | Acc: 93.600% (32292/34500) [========================================================>.......................]  Step: 67ms | Tot: 23s554ms | Loss: 0.1953 | Acc: 93.553% (32837/35100) [========================================================>.......................]  Step: 67ms | Tot: 23s621ms | Loss: 0.1952 | Acc: 93.548% (32929/35200) [=========================================================>......................]  Step: 67ms | Tot: 24s89ms | Loss: 0.1958 | Acc: 93.515% (33572/35900) [==========================================================>.....................]  Step: 67ms | Tot: 24s488ms | Loss: 0.1959 | Acc: 93.493% (34125/36500) [===========================================================>....................]  Step: 65ms | Tot: 24s895ms | Loss: 0.1963 | Acc: 93.488% (34684/37100) [=======================================================================>........]  Step: 67ms | Tot: 29s890ms | Loss: 0.2013 | Acc: 93.288% (41513/44500) [=============================================================================>..]  Step: 66ms | Tot: 32s552ms | Loss: 0.2023 | Acc: 93.249% (45226/48500) [==============================================================================>.]  Step: 66ms | Tot: 32s819ms | Loss: 0.2023 | Acc: 93.256% (45602/48900) [==============================================================================>.]  Step: 66ms | Tot: 32s954ms | Loss: 0.2024 | Acc: 93.259% (45790/49100)\n",
      "(101.51589485257864, 0.9324)\n",
      "\n",
      "===> epoch: 62/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s311ms | Loss: 0.1988 | Acc: 93.364% (46682/50000)>............................................................................]  Step: 66ms | Tot: 1s199ms | Loss: 0.1892 | Acc: 93.842% (1783/1900) [====>...........................................................................]  Step: 66ms | Tot: 1s598ms | Loss: 0.1822 | Acc: 94.000% (2350/2500) [======>.........................................................................]  Step: 67ms | Tot: 2s732ms | Loss: 0.1809 | Acc: 93.929% (3945/4200) [=======>........................................................................]  Step: 66ms | Tot: 2s865ms | Loss: 0.1782 | Acc: 94.023% (4137/4400) [=================>..............................................................]  Step: 67ms | Tot: 7s178ms | Loss: 0.1867 | Acc: 93.807% (10225/10900) [=================>..............................................................]  Step: 66ms | Tot: 7s245ms | Loss: 0.1857 | Acc: 93.845% (10323/11000) [===================>............................................................]  Step: 66ms | Tot: 7s976ms | Loss: 0.1863 | Acc: 93.810% (11351/12100) [=========================>......................................................]  Step: 67ms | Tot: 10s417ms | Loss: 0.1804 | Acc: 93.943% (14843/15800) [=========================>......................................................]  Step: 66ms | Tot: 10s483ms | Loss: 0.1799 | Acc: 93.956% (14939/15900) [============================>...................................................]  Step: 66ms | Tot: 11s807ms | Loss: 0.1803 | Acc: 93.955% (16818/17900) [=======================================>........................................]  Step: 66ms | Tot: 16s377ms | Loss: 0.1899 | Acc: 93.702% (23238/24800) [==========================================>.....................................]  Step: 66ms | Tot: 17s380ms | Loss: 0.1888 | Acc: 93.749% (24656/26300) [==========================================>.....................................]  Step: 67ms | Tot: 17s447ms | Loss: 0.1886 | Acc: 93.761% (24753/26400) [============================================>...................................]  Step: 67ms | Tot: 18s448ms | Loss: 0.1881 | Acc: 93.731% (26151/27900) [================================================>...............................]  Step: 66ms | Tot: 19s847ms | Loss: 0.1893 | Acc: 93.670% (28101/30000) [=================================================>..............................]  Step: 66ms | Tot: 20s314ms | Loss: 0.1896 | Acc: 93.671% (28757/30700) [=================================================>..............................]  Step: 67ms | Tot: 20s381ms | Loss: 0.1896 | Acc: 93.675% (28852/30800) [===================================================>............................]  Step: 67ms | Tot: 21s248ms | Loss: 0.1897 | Acc: 93.629% (30055/32100) [=====================================================>..........................]  Step: 67ms | Tot: 21s980ms | Loss: 0.1906 | Acc: 93.617% (31081/33200) [=====================================================>..........................]  Step: 66ms | Tot: 22s114ms | Loss: 0.1906 | Acc: 93.614% (31267/33400) [=========================================================>......................]  Step: 67ms | Tot: 23s979ms | Loss: 0.1910 | Acc: 93.616% (33889/36200) [===========================================================>....................]  Step: 66ms | Tot: 24s513ms | Loss: 0.1915 | Acc: 93.627% (34642/37000) [============================================================>...................]  Step: 67ms | Tot: 25s116ms | Loss: 0.1915 | Acc: 93.609% (35478/37900) [==================================================================>.............]  Step: 67ms | Tot: 27s638ms | Loss: 0.1955 | Acc: 93.505% (38898/41600) [==================================================================>.............]  Step: 66ms | Tot: 27s772ms | Loss: 0.1954 | Acc: 93.505% (39085/41800) [===================================================================>............]  Step: 67ms | Tot: 27s839ms | Loss: 0.1953 | Acc: 93.504% (39178/41900) [=====================================================================>..........]  Step: 66ms | Tot: 28s912ms | Loss: 0.1950 | Acc: 93.526% (40684/43500) [======================================================================>.........]  Step: 66ms | Tot: 29s313ms | Loss: 0.1951 | Acc: 93.508% (41237/44100) [=============================================================================>..]  Step: 67ms | Tot: 32s317ms | Loss: 0.1978 | Acc: 93.384% (45291/48500)\n",
      "(99.40745880827308, 0.93364)\n",
      "\n",
      "===> epoch: 63/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s318ms | Loss: 0.2008 | Acc: 93.410% (46705/50000)>............................................................................]  Step: 67ms | Tot: 1s404ms | Loss: 0.2046 | Acc: 93.045% (2047/2200) [========>.......................................................................]  Step: 66ms | Tot: 3s520ms | Loss: 0.1848 | Acc: 93.698% (4966/5300) [========>.......................................................................]  Step: 67ms | Tot: 3s587ms | Loss: 0.1839 | Acc: 93.722% (5061/5400) [=========>......................................................................]  Step: 65ms | Tot: 3s921ms | Loss: 0.1853 | Acc: 93.644% (5525/5900) [==========>.....................................................................]  Step: 66ms | Tot: 4s388ms | Loss: 0.1839 | Acc: 93.682% (6183/6600) [===========>....................................................................]  Step: 65ms | Tot: 4s722ms | Loss: 0.1861 | Acc: 93.549% (6642/7100) [============>...................................................................]  Step: 66ms | Tot: 5s190ms | Loss: 0.1861 | Acc: 93.564% (7298/7800) [=============>..................................................................]  Step: 67ms | Tot: 5s726ms | Loss: 0.1798 | Acc: 93.744% (8062/8600) [==============>.................................................................]  Step: 66ms | Tot: 5s927ms | Loss: 0.1787 | Acc: 93.708% (8340/8900) [===============>................................................................]  Step: 67ms | Tot: 6s396ms | Loss: 0.1771 | Acc: 93.802% (9005/9600) [===============>................................................................]  Step: 67ms | Tot: 6s530ms | Loss: 0.1770 | Acc: 93.786% (9191/9800) [===============>................................................................]  Step: 67ms | Tot: 6s597ms | Loss: 0.1768 | Acc: 93.808% (9287/9900) [================>...............................................................]  Step: 66ms | Tot: 6s664ms | Loss: 0.1774 | Acc: 93.800% (9380/10000) [=================>..............................................................]  Step: 66ms | Tot: 7s396ms | Loss: 0.1791 | Acc: 93.802% (10412/11100) [==================>.............................................................]  Step: 66ms | Tot: 7s796ms | Loss: 0.1823 | Acc: 93.701% (10963/11700) [===================>............................................................]  Step: 67ms | Tot: 8s264ms | Loss: 0.1819 | Acc: 93.750% (11625/12400) [========================>.......................................................]  Step: 65ms | Tot: 9s989ms | Loss: 0.1817 | Acc: 93.880% (14082/15000) [========================>.......................................................]  Step: 66ms | Tot: 10s325ms | Loss: 0.1811 | Acc: 93.897% (14554/15500) [==========================>.....................................................]  Step: 67ms | Tot: 11s59ms | Loss: 0.1779 | Acc: 93.970% (15599/16600) [==========================>.....................................................]  Step: 66ms | Tot: 11s193ms | Loss: 0.1780 | Acc: 93.976% (15788/16800) [=============================>..................................................]  Step: 66ms | Tot: 12s193ms | Loss: 0.1784 | Acc: 94.044% (17210/18300) [=============================>..................................................]  Step: 66ms | Tot: 12s260ms | Loss: 0.1786 | Acc: 94.033% (17302/18400) [=============================>..................................................]  Step: 67ms | Tot: 12s393ms | Loss: 0.1790 | Acc: 94.022% (17488/18600) [==============================>.................................................]  Step: 67ms | Tot: 12s728ms | Loss: 0.1794 | Acc: 94.058% (17965/19100) [==================================>.............................................]  Step: 67ms | Tot: 14s523ms | Loss: 0.1849 | Acc: 93.881% (20466/21800) [=======================================>........................................]  Step: 66ms | Tot: 16s321ms | Loss: 0.1871 | Acc: 93.763% (22972/24500) [==========================================>.....................................]  Step: 67ms | Tot: 17s721ms | Loss: 0.1879 | Acc: 93.726% (24931/26600) [==========================================>.....................................]  Step: 67ms | Tot: 17s788ms | Loss: 0.1879 | Acc: 93.727% (25025/26700) [===========================================>....................................]  Step: 66ms | Tot: 17s922ms | Loss: 0.1882 | Acc: 93.714% (25209/26900) [===========================================>....................................]  Step: 67ms | Tot: 18s257ms | Loss: 0.1890 | Acc: 93.690% (25671/27400) [============================================>...................................]  Step: 66ms | Tot: 18s656ms | Loss: 0.1902 | Acc: 93.632% (26217/28000) [==============================================>.................................]  Step: 67ms | Tot: 19s396ms | Loss: 0.1911 | Acc: 93.629% (27246/29100) [==============================================>.................................]  Step: 65ms | Tot: 19s462ms | Loss: 0.1915 | Acc: 93.627% (27339/29200) [==================================================>.............................]  Step: 67ms | Tot: 20s991ms | Loss: 0.1961 | Acc: 93.505% (29454/31500) [=====================================================>..........................]  Step: 67ms | Tot: 22s129ms | Loss: 0.1989 | Acc: 93.407% (31011/33200) [==============================================================>.................]  Step: 67ms | Tot: 26s197ms | Loss: 0.1971 | Acc: 93.506% (36748/39300) [===============================================================>................]  Step: 65ms | Tot: 26s329ms | Loss: 0.1968 | Acc: 93.516% (36939/39500) [=================================================================>..............]  Step: 67ms | Tot: 27s396ms | Loss: 0.1979 | Acc: 93.496% (38427/41100) [==================================================================>.............]  Step: 67ms | Tot: 27s797ms | Loss: 0.1980 | Acc: 93.499% (38989/41700) [=====================================================================>..........]  Step: 67ms | Tot: 28s995ms | Loss: 0.1985 | Acc: 93.474% (40661/43500) [==========================================================================>.....]  Step: 66ms | Tot: 30s987ms | Loss: 0.1994 | Acc: 93.439% (43449/46500) [=============================================================================>..]  Step: 66ms | Tot: 32s187ms | Loss: 0.1993 | Acc: 93.418% (45121/48300) [===============================================================================>]  Step: 66ms | Tot: 33s120ms | Loss: 0.2002 | Acc: 93.419% (46429/49700)\n",
      "(100.38757040351629, 0.9341)\n",
      "\n",
      "===> epoch: 64/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s478ms | Loss: 0.1950 | Acc: 93.580% (46790/50000).......................................................................]  Step: 65ms | Tot: 132ms | Loss: 0.1754 | Acc: 93.333% (280/300) [==>.............................................................................]  Step: 66ms | Tot: 797ms | Loss: 0.1905 | Acc: 93.769% (1219/1300) [===>............................................................................]  Step: 67ms | Tot: 1s330ms | Loss: 0.1949 | Acc: 93.762% (1969/2100) [=====>..........................................................................]  Step: 67ms | Tot: 2s130ms | Loss: 0.1885 | Acc: 93.939% (3100/3300) [======>.........................................................................]  Step: 66ms | Tot: 2s530ms | Loss: 0.1796 | Acc: 94.154% (3672/3900) [==========>.....................................................................]  Step: 67ms | Tot: 4s461ms | Loss: 0.1716 | Acc: 94.088% (6398/6800) [============>...................................................................]  Step: 66ms | Tot: 4s928ms | Loss: 0.1744 | Acc: 94.107% (7058/7500) [=============>..................................................................]  Step: 67ms | Tot: 5s530ms | Loss: 0.1747 | Acc: 94.095% (7904/8400) [==================>.............................................................]  Step: 67ms | Tot: 7s660ms | Loss: 0.1821 | Acc: 93.819% (10883/11600) [====================>...........................................................]  Step: 67ms | Tot: 8s394ms | Loss: 0.1837 | Acc: 93.693% (11899/12700) [=====================>..........................................................]  Step: 67ms | Tot: 8s794ms | Loss: 0.1836 | Acc: 93.699% (12462/13300) [=======================>........................................................]  Step: 67ms | Tot: 9s591ms | Loss: 0.1858 | Acc: 93.669% (13582/14500) [============================>...................................................]  Step: 67ms | Tot: 11s588ms | Loss: 0.1814 | Acc: 93.851% (16424/17500) [============================>...................................................]  Step: 66ms | Tot: 11s655ms | Loss: 0.1813 | Acc: 93.847% (16517/17600) [===================================>............................................]  Step: 66ms | Tot: 14s790ms | Loss: 0.1827 | Acc: 93.807% (20919/22300) [====================================>...........................................]  Step: 66ms | Tot: 15s258ms | Loss: 0.1834 | Acc: 93.809% (21576/23000) [========================================>.......................................]  Step: 66ms | Tot: 16s586ms | Loss: 0.1834 | Acc: 93.796% (23449/25000) [========================================>.......................................]  Step: 66ms | Tot: 16s920ms | Loss: 0.1832 | Acc: 93.835% (23928/25500) [=============================================>..................................]  Step: 67ms | Tot: 18s835ms | Loss: 0.1828 | Acc: 93.890% (26571/28300) [==================================================>.............................]  Step: 66ms | Tot: 21s216ms | Loss: 0.1876 | Acc: 93.814% (29833/31800) [===================================================>............................]  Step: 66ms | Tot: 21s417ms | Loss: 0.1880 | Acc: 93.807% (30112/32100) [================================================================>...............]  Step: 66ms | Tot: 26s988ms | Loss: 0.1900 | Acc: 93.747% (37780/40300) [====================================================================>...........]  Step: 67ms | Tot: 28s805ms | Loss: 0.1905 | Acc: 93.705% (40293/43000) [=====================================================================>..........]  Step: 66ms | Tot: 28s938ms | Loss: 0.1905 | Acc: 93.704% (40480/43200) [======================================================================>.........]  Step: 67ms | Tot: 29s671ms | Loss: 0.1920 | Acc: 93.675% (41498/44300) [=======================================================================>........]  Step: 65ms | Tot: 29s804ms | Loss: 0.1920 | Acc: 93.676% (41686/44500) [==========================================================================>.....]  Step: 67ms | Tot: 31s212ms | Loss: 0.1946 | Acc: 93.590% (43613/46600) [==========================================================================>.....]  Step: 66ms | Tot: 31s346ms | Loss: 0.1947 | Acc: 93.577% (43794/46800) [============================================================================>...]  Step: 66ms | Tot: 32s212ms | Loss: 0.1940 | Acc: 93.607% (45025/48100) [==============================================================================>.]  Step: 66ms | Tot: 32s948ms | Loss: 0.1944 | Acc: 93.593% (46048/49200)\n",
      "(97.49723187088966, 0.9358)\n",
      "\n",
      "===> epoch: 65/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 69ms | Tot: 33s424ms | Loss: 0.1801 | Acc: 94.056% (47028/50000)........................................................................]  Step: 66ms | Tot: 201ms | Loss: 0.1777 | Acc: 95.000% (380/400) [=>..............................................................................]  Step: 67ms | Tot: 401ms | Loss: 0.1821 | Acc: 94.000% (658/700) [====>...........................................................................]  Step: 66ms | Tot: 1s742ms | Loss: 0.1780 | Acc: 94.222% (2544/2700) [========>.......................................................................]  Step: 68ms | Tot: 3s271ms | Loss: 0.1654 | Acc: 94.480% (4724/5000) [=========>......................................................................]  Step: 66ms | Tot: 4s68ms | Loss: 0.1623 | Acc: 94.581% (5864/6200) [===============>................................................................]  Step: 66ms | Tot: 6s608ms | Loss: 0.1711 | Acc: 94.515% (9357/9900) [==================>.............................................................]  Step: 68ms | Tot: 7s605ms | Loss: 0.1731 | Acc: 94.404% (10762/11400) [======================>.........................................................]  Step: 67ms | Tot: 9s334ms | Loss: 0.1719 | Acc: 94.400% (13216/14000) [=======================>........................................................]  Step: 67ms | Tot: 9s667ms | Loss: 0.1714 | Acc: 94.407% (13689/14500) [=======================>........................................................]  Step: 66ms | Tot: 9s800ms | Loss: 0.1718 | Acc: 94.388% (13875/14700) [========================>.......................................................]  Step: 66ms | Tot: 10s134ms | Loss: 0.1718 | Acc: 94.382% (14346/15200) [===========================>....................................................]  Step: 67ms | Tot: 11s330ms | Loss: 0.1735 | Acc: 94.276% (16027/17000) [======================================>.........................................]  Step: 67ms | Tot: 16s196ms | Loss: 0.1740 | Acc: 94.267% (22907/24300) [========================================>.......................................]  Step: 67ms | Tot: 16s862ms | Loss: 0.1741 | Acc: 94.261% (23848/25300) [===========================================>....................................]  Step: 66ms | Tot: 18s66ms | Loss: 0.1729 | Acc: 94.306% (25557/27100) [==============================================>.................................]  Step: 66ms | Tot: 19s396ms | Loss: 0.1727 | Acc: 94.292% (27439/29100) [==================================================>.............................]  Step: 67ms | Tot: 20s929ms | Loss: 0.1750 | Acc: 94.210% (29582/31400) [==================================================>.............................]  Step: 66ms | Tot: 20s996ms | Loss: 0.1750 | Acc: 94.210% (29676/31500) [=======================================================>........................]  Step: 67ms | Tot: 23s188ms | Loss: 0.1743 | Acc: 94.187% (32777/34800) [=========================================================>......................]  Step: 66ms | Tot: 23s789ms | Loss: 0.1759 | Acc: 94.143% (33609/35700) [=========================================================>......................]  Step: 67ms | Tot: 24s55ms | Loss: 0.1768 | Acc: 94.119% (33977/36100) [==========================================================>.....................]  Step: 66ms | Tot: 24s456ms | Loss: 0.1766 | Acc: 94.120% (34542/36700) [===========================================================>....................]  Step: 66ms | Tot: 24s590ms | Loss: 0.1764 | Acc: 94.127% (34733/36900) [============================================================>...................]  Step: 67ms | Tot: 25s329ms | Loss: 0.1769 | Acc: 94.116% (35764/38000) [============================================================>...................]  Step: 66ms | Tot: 25s395ms | Loss: 0.1769 | Acc: 94.115% (35858/38100) [================================================================>...............]  Step: 67ms | Tot: 26s926ms | Loss: 0.1772 | Acc: 94.124% (38026/40400) [================================================================>...............]  Step: 66ms | Tot: 26s993ms | Loss: 0.1773 | Acc: 94.126% (38121/40500) [================================================================>...............]  Step: 66ms | Tot: 27s60ms | Loss: 0.1772 | Acc: 94.131% (38217/40600) [==================================================================>.............]  Step: 67ms | Tot: 27s728ms | Loss: 0.1778 | Acc: 94.103% (39147/41600) [=====================================================================>..........]  Step: 65ms | Tot: 29s121ms | Loss: 0.1786 | Acc: 94.087% (41116/43700) [==========================================================================>.....]  Step: 67ms | Tot: 30s855ms | Loss: 0.1789 | Acc: 94.078% (43558/46300) [==========================================================================>.....]  Step: 66ms | Tot: 31s56ms | Loss: 0.1787 | Acc: 94.088% (43845/46600)\n",
      "(90.06458907946944, 0.94056)\n",
      "\n",
      "===> epoch: 66/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s998ms | Loss: 0.1792 | Acc: 94.142% (47071/50000)>............................................................................]  Step: 66ms | Tot: 1s555ms | Loss: 0.2031 | Acc: 93.625% (2247/2400) [========>.......................................................................]  Step: 66ms | Tot: 3s301ms | Loss: 0.1956 | Acc: 93.620% (4681/5000) [===============>................................................................]  Step: 66ms | Tot: 6s403ms | Loss: 0.1858 | Acc: 93.979% (9022/9600) [================>...............................................................]  Step: 67ms | Tot: 6s740ms | Loss: 0.1845 | Acc: 93.990% (9493/10100) [================>...............................................................]  Step: 66ms | Tot: 7s8ms | Loss: 0.1846 | Acc: 93.962% (9866/10500) [========================>.......................................................]  Step: 67ms | Tot: 10s192ms | Loss: 0.1740 | Acc: 94.309% (14335/15200) [============================>...................................................]  Step: 66ms | Tot: 12s189ms | Loss: 0.1738 | Acc: 94.320% (17072/18100) [=============================>..................................................]  Step: 66ms | Tot: 12s390ms | Loss: 0.1739 | Acc: 94.315% (17354/18400) [===============================>................................................]  Step: 66ms | Tot: 13s52ms | Loss: 0.1734 | Acc: 94.278% (18290/19400) [===============================================>................................]  Step: 66ms | Tot: 20s122ms | Loss: 0.1754 | Acc: 94.300% (28007/29700) [====================================================>...........................]  Step: 67ms | Tot: 22s274ms | Loss: 0.1754 | Acc: 94.256% (30916/32800) [====================================================>...........................]  Step: 67ms | Tot: 22s341ms | Loss: 0.1752 | Acc: 94.267% (31014/32900) [=====================================================>..........................]  Step: 66ms | Tot: 22s743ms | Loss: 0.1744 | Acc: 94.293% (31588/33500) [=====================================================>..........................]  Step: 66ms | Tot: 22s809ms | Loss: 0.1741 | Acc: 94.304% (31686/33600) [=============================================================>..................]  Step: 66ms | Tot: 25s941ms | Loss: 0.1758 | Acc: 94.270% (36011/38200) [==============================================================>.................]  Step: 67ms | Tot: 26s406ms | Loss: 0.1752 | Acc: 94.290% (36679/38900) [==============================================================>.................]  Step: 67ms | Tot: 26s474ms | Loss: 0.1750 | Acc: 94.297% (36776/39000) [===============================================================>................]  Step: 67ms | Tot: 26s809ms | Loss: 0.1747 | Acc: 94.306% (37251/39500) [===============================================================>................]  Step: 67ms | Tot: 26s876ms | Loss: 0.1746 | Acc: 94.311% (37347/39600) [==================================================================>.............]  Step: 66ms | Tot: 28s403ms | Loss: 0.1749 | Acc: 94.299% (39417/41800) [========================================================================>.......]  Step: 68ms | Tot: 30s801ms | Loss: 0.1764 | Acc: 94.234% (42688/45300) [===========================================================================>....]  Step: 67ms | Tot: 32s146ms | Loss: 0.1773 | Acc: 94.218% (44565/47300) [============================================================================>...]  Step: 67ms | Tot: 32s280ms | Loss: 0.1778 | Acc: 94.208% (44749/47500)\n",
      "(89.62295105680823, 0.94142)\n",
      "\n",
      "===> epoch: 67/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 34s349ms | Loss: 0.1834 | Acc: 93.990% (46995/50000)===>.........................................................................]  Step: 67ms | Tot: 2s607ms | Loss: 0.1343 | Acc: 95.795% (3736/3900) [======>.........................................................................]  Step: 66ms | Tot: 2s740ms | Loss: 0.1383 | Acc: 95.683% (3923/4100) [========>.......................................................................]  Step: 68ms | Tot: 3s679ms | Loss: 0.1331 | Acc: 95.764% (5267/5500) [=========>......................................................................]  Step: 68ms | Tot: 4s14ms | Loss: 0.1347 | Acc: 95.633% (5738/6000) [=============>..................................................................]  Step: 67ms | Tot: 5s573ms | Loss: 0.1392 | Acc: 95.373% (7916/8300) [==============>.................................................................]  Step: 67ms | Tot: 6s244ms | Loss: 0.1494 | Acc: 95.151% (8849/9300) [=============================================>..................................]  Step: 67ms | Tot: 19s969ms | Loss: 0.1729 | Acc: 94.282% (26776/28400) [==============================================>.................................]  Step: 67ms | Tot: 20s235ms | Loss: 0.1731 | Acc: 94.278% (27152/28800) [==============================================>.................................]  Step: 66ms | Tot: 20s302ms | Loss: 0.1733 | Acc: 94.266% (27243/28900) [=========================================================>......................]  Step: 67ms | Tot: 24s920ms | Loss: 0.1773 | Acc: 94.087% (33683/35800) [==========================================================>.....................]  Step: 66ms | Tot: 25s388ms | Loss: 0.1775 | Acc: 94.093% (34344/36500) [=============================================================>..................]  Step: 66ms | Tot: 26s782ms | Loss: 0.1771 | Acc: 94.132% (36335/38600) [================================================================>...............]  Step: 66ms | Tot: 27s713ms | Loss: 0.1792 | Acc: 94.085% (37634/40000) [==================================================================>.............]  Step: 67ms | Tot: 28s581ms | Loss: 0.1804 | Acc: 94.044% (38840/41300) [=========================================================================>......]  Step: 67ms | Tot: 31s756ms | Loss: 0.1835 | Acc: 93.993% (43331/46100) [==========================================================================>.....]  Step: 66ms | Tot: 31s889ms | Loss: 0.1833 | Acc: 94.002% (43523/46300) [============================================================================>...]  Step: 68ms | Tot: 33s24ms | Loss: 0.1830 | Acc: 94.025% (45132/48000) [=============================================================================>..]  Step: 66ms | Tot: 33s156ms | Loss: 0.1827 | Acc: 94.031% (45323/48200)\n",
      "(91.69850404560566, 0.9399)\n",
      "\n",
      "===> epoch: 68/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s369ms | Loss: 0.1875 | Acc: 93.994% (46997/50000)...........................................................................]  Step: 68ms | Tot: 864ms | Loss: 0.1704 | Acc: 94.786% (1327/1400) [=====>..........................................................................]  Step: 66ms | Tot: 2s192ms | Loss: 0.1838 | Acc: 94.324% (3207/3400) [=======>........................................................................]  Step: 67ms | Tot: 2s920ms | Loss: 0.1811 | Acc: 94.400% (4248/4500) [================>...............................................................]  Step: 67ms | Tot: 6s908ms | Loss: 0.1858 | Acc: 94.250% (9802/10400) [==================>.............................................................]  Step: 66ms | Tot: 7s722ms | Loss: 0.1824 | Acc: 94.336% (10943/11600) [====================>...........................................................]  Step: 67ms | Tot: 8s756ms | Loss: 0.1848 | Acc: 94.260% (12348/13100) [======================>.........................................................]  Step: 65ms | Tot: 9s424ms | Loss: 0.1814 | Acc: 94.383% (13308/14100) [=============================>..................................................]  Step: 68ms | Tot: 12s181ms | Loss: 0.1790 | Acc: 94.462% (17192/18200) [=============================>..................................................]  Step: 65ms | Tot: 12s313ms | Loss: 0.1784 | Acc: 94.484% (17385/18400) [=====================================>..........................................]  Step: 66ms | Tot: 15s705ms | Loss: 0.1772 | Acc: 94.489% (22205/23500) [===========================================>....................................]  Step: 67ms | Tot: 18s97ms | Loss: 0.1767 | Acc: 94.424% (25589/27100) [============================================>...................................]  Step: 67ms | Tot: 18s629ms | Loss: 0.1768 | Acc: 94.427% (26345/27900) [============================================>...................................]  Step: 65ms | Tot: 18s695ms | Loss: 0.1770 | Acc: 94.429% (26440/28000) [======================================================>.........................]  Step: 66ms | Tot: 22s531ms | Loss: 0.1820 | Acc: 94.201% (31840/33800) [=======================================================>........................]  Step: 65ms | Tot: 23s62ms | Loss: 0.1826 | Acc: 94.165% (32581/34600) [========================================================>.......................]  Step: 66ms | Tot: 23s463ms | Loss: 0.1827 | Acc: 94.170% (33148/35200) [========================================================>.......................]  Step: 67ms | Tot: 23s597ms | Loss: 0.1828 | Acc: 94.164% (33334/35400) [==========================================================>.....................]  Step: 66ms | Tot: 24s463ms | Loss: 0.1829 | Acc: 94.161% (34557/36700) [============================================================>...................]  Step: 66ms | Tot: 24s997ms | Loss: 0.1833 | Acc: 94.147% (35305/37500) [====================================================================>...........]  Step: 66ms | Tot: 28s649ms | Loss: 0.1889 | Acc: 93.998% (40325/42900)\n",
      "(93.75414865091443, 0.93994)\n",
      "\n",
      "===> epoch: 69/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s158ms | Loss: 0.1751 | Acc: 94.086% (47043/50000)==>..........................................................................]  Step: 66ms | Tot: 2s316ms | Loss: 0.1845 | Acc: 93.722% (3374/3600) [===============>................................................................]  Step: 66ms | Tot: 6s355ms | Loss: 0.1767 | Acc: 94.206% (9138/9700) [============================>...................................................]  Step: 66ms | Tot: 11s907ms | Loss: 0.1696 | Acc: 94.343% (17076/18100) [==================================>.............................................]  Step: 67ms | Tot: 14s288ms | Loss: 0.1732 | Acc: 94.267% (20456/21700) [======================================>.........................................]  Step: 67ms | Tot: 15s814ms | Loss: 0.1712 | Acc: 94.271% (22625/24000) [=======================================>........................................]  Step: 67ms | Tot: 16s280ms | Loss: 0.1714 | Acc: 94.271% (23285/24700) [====================================================================>...........]  Step: 66ms | Tot: 28s227ms | Loss: 0.1705 | Acc: 94.239% (40240/42700) [=====================================================================>..........]  Step: 66ms | Tot: 28s761ms | Loss: 0.1712 | Acc: 94.218% (40985/43500) [==========================================================================>.....]  Step: 66ms | Tot: 30s801ms | Loss: 0.1735 | Acc: 94.129% (43770/46500)\n",
      "(87.55966344475746, 0.94086)\n",
      "\n",
      "===> epoch: 70/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s240ms | Loss: 0.1814 | Acc: 94.150% (47075/50000)============>................................................................]  Step: 66ms | Tot: 6s344ms | Loss: 0.1859 | Acc: 94.323% (9055/9600) [=================>..............................................................]  Step: 66ms | Tot: 7s280ms | Loss: 0.1922 | Acc: 94.082% (10349/11000) [==========================>.....................................................]  Step: 68ms | Tot: 11s63ms | Loss: 0.1871 | Acc: 94.138% (15721/16700) [==================================>.............................................]  Step: 67ms | Tot: 14s399ms | Loss: 0.1864 | Acc: 94.056% (20316/21600) [================================================>...............................]  Step: 67ms | Tot: 20s300ms | Loss: 0.1854 | Acc: 94.059% (28688/30500) [===================================================================>............]  Step: 67ms | Tot: 28s169ms | Loss: 0.1810 | Acc: 94.175% (39930/42400)\n",
      "(90.71096659451723, 0.9415)\n",
      "\n",
      "===> epoch: 71/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s194ms | Loss: 0.1715 | Acc: 94.522% (47261/50000)====>........................................................................]  Step: 67ms | Tot: 2s975ms | Loss: 0.1496 | Acc: 95.109% (4375/4600) [========>.......................................................................]  Step: 66ms | Tot: 3s636ms | Loss: 0.1496 | Acc: 95.143% (5328/5600) [============>...................................................................]  Step: 67ms | Tot: 4s893ms | Loss: 0.1465 | Acc: 95.213% (7141/7500) [=======================>........................................................]  Step: 67ms | Tot: 9s688ms | Loss: 0.1489 | Acc: 95.163% (13989/14700) [===========================>....................................................]  Step: 66ms | Tot: 11s288ms | Loss: 0.1501 | Acc: 95.123% (16266/17100) [===========================>....................................................]  Step: 66ms | Tot: 11s423ms | Loss: 0.1517 | Acc: 95.075% (16448/17300) [==================================>.............................................]  Step: 67ms | Tot: 14s155ms | Loss: 0.1565 | Acc: 94.944% (20318/21400) [=================================================>..............................]  Step: 66ms | Tot: 20s522ms | Loss: 0.1607 | Acc: 94.845% (29402/31000) [=====================================================>..........................]  Step: 66ms | Tot: 22s252ms | Loss: 0.1614 | Acc: 94.810% (31856/33600) [===========================================================>....................]  Step: 66ms | Tot: 24s547ms | Loss: 0.1600 | Acc: 94.835% (35089/37000) [=====================================================================>..........]  Step: 66ms | Tot: 28s757ms | Loss: 0.1654 | Acc: 94.667% (40991/43300) [=====================================================================>..........]  Step: 67ms | Tot: 28s891ms | Loss: 0.1655 | Acc: 94.669% (41181/43500) [=============================================================================>..]  Step: 66ms | Tot: 32s138ms | Loss: 0.1709 | Acc: 94.527% (45751/48400)\n",
      "(85.72873836569488, 0.94522)\n",
      "\n",
      "===> epoch: 72/100\n",
      "train:\n",
      " 500/500 [===============================================================================>]  Step: 66ms | Tot: 33s246ms | Loss: 0.1791 | Acc: 94.226% (47019/49900)==>...........................................................................]  Step: 66ms | Tot: 1s839ms | Loss: 0.1509 | Acc: 94.929% (2658/2800) [====>...........................................................................]  Step: 66ms | Tot: 1s906ms | Loss: 0.1500 | Acc: 94.966% (2754/2900) [====>...........................................................................]  Step: 66ms | Tot: 2s40ms | Loss: 0.1500 | Acc: 94.935% (2943/3100) [=========>......................................................................]  Step: 66ms | Tot: 4s80ms | Loss: 0.1474 | Acc: 94.968% (5888/6200) [==============>.................................................................]  Step: 67ms | Tot: 5s996ms | Loss: 0.1652 | Acc: 94.622% (8516/9000) [==============>.................................................................]  Step: 66ms | Tot: 6s62ms | Loss: 0.1647 | Acc: 94.637% (8612/9100) [=====================================>..........................................]  Step: 67ms | Tot: 15s722ms | Loss: 0.1790 | Acc: 94.135% (22310/23700) [==================================================>.............................]  Step: 66ms | Tot: 20s976ms | Loss: 0.1816 | Acc: 94.079% (29635/31500) [==================================================>.............................]  Step: 66ms | Tot: 21s43ms | Loss: 0.1812 | Acc: 94.092% (29733/31600) [==============================================================>.................]  Step: 67ms | Tot: 26s110ms | Loss: 0.1790 | Acc: 94.222% (36935/39200) [===============================================================>................]  Step: 66ms | Tot: 26s509ms | Loss: 0.1791 | Acc: 94.221% (37500/39800) [==================================================================>.............]  Step: 66ms | Tot: 27s510ms | Loss: 0.1780 | Acc: 94.254% (38927/41300) [==================================================================>.............]  Step: 67ms | Tot: 27s779ms | Loss: 0.1775 | Acc: 94.271% (39311/41700) [========================================================================>.......]  Step: 66ms | Tot: 30s312ms | Loss: 0.1779 | Acc: 94.279% (42897/45500) [==========================================================================>.....]  Step: 66ms | Tot: 30s846ms | Loss: 0.1788 | Acc: 94.244% (43635/46300) [===========================================================================>....]  Step: 65ms | Tot: 31s378ms | Loss: 0.1786 | Acc: 94.238% (44386/47100) [=============================================================================>..]  Step: 67ms | Tot: 32s379ms | Loss: 0.1791 | Acc: 94.224% (45793/48600) [==============================================================================>.]  Step: 69ms | Tot: 32s649ms | Loss: 0.1796 | Acc: 94.214% (46165/49000) [================================================================================>]  Step: 67ms | Tot: 33s313ms | Loss: 0.1790 | Acc: 94.228% (47114/50000)\n",
      "(89.51385069265962, 0.94228)\n",
      "\n",
      "===> epoch: 73/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s268ms | Loss: 0.1706 | Acc: 94.544% (47272/50000)........................................................................]  Step: 66ms | Tot: 531ms | Loss: 0.1803 | Acc: 95.222% (857/900) [=>..............................................................................]  Step: 66ms | Tot: 598ms | Loss: 0.1776 | Acc: 95.200% (952/1000) [====>...........................................................................]  Step: 66ms | Tot: 1s592ms | Loss: 0.1581 | Acc: 94.880% (2372/2500) [=====>..........................................................................]  Step: 66ms | Tot: 2s260ms | Loss: 0.1507 | Acc: 95.143% (3330/3500) [======>.........................................................................]  Step: 66ms | Tot: 2s528ms | Loss: 0.1589 | Acc: 94.744% (3695/3900) [======>.........................................................................]  Step: 67ms | Tot: 2s661ms | Loss: 0.1591 | Acc: 94.683% (3882/4100) [======>.........................................................................]  Step: 67ms | Tot: 2s728ms | Loss: 0.1587 | Acc: 94.738% (3979/4200) [=======>........................................................................]  Step: 66ms | Tot: 2s995ms | Loss: 0.1594 | Acc: 94.761% (4359/4600) [==========>.....................................................................]  Step: 68ms | Tot: 4s125ms | Loss: 0.1638 | Acc: 94.810% (5973/6300) [==========>.....................................................................]  Step: 66ms | Tot: 4s258ms | Loss: 0.1636 | Acc: 94.754% (6159/6500) [===========>....................................................................]  Step: 67ms | Tot: 4s725ms | Loss: 0.1622 | Acc: 94.736% (6821/7200) [============>...................................................................]  Step: 66ms | Tot: 5s191ms | Loss: 0.1611 | Acc: 94.785% (7488/7900) [=================>..............................................................]  Step: 66ms | Tot: 7s125ms | Loss: 0.1586 | Acc: 95.065% (10267/10800) [====================>...........................................................]  Step: 67ms | Tot: 8s654ms | Loss: 0.1587 | Acc: 95.076% (12455/13100) [=====================>..........................................................]  Step: 67ms | Tot: 8s787ms | Loss: 0.1579 | Acc: 95.090% (12647/13300) [======================================================>.........................]  Step: 67ms | Tot: 22s681ms | Loss: 0.1686 | Acc: 94.625% (32267/34100) [=====================================================================>..........]  Step: 65ms | Tot: 28s768ms | Loss: 0.1714 | Acc: 94.544% (40843/43200) [===========================================================================>....]  Step: 67ms | Tot: 31s547ms | Loss: 0.1718 | Acc: 94.519% (44802/47400)\n",
      "(85.3102855309844, 0.94544)\n",
      "\n",
      "===> epoch: 74/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s465ms | Loss: 0.1642 | Acc: 94.800% (47400/50000)=>...........................................................................]  Step: 67ms | Tot: 1s922ms | Loss: 0.1265 | Acc: 96.138% (2788/2900) [====>...........................................................................]  Step: 67ms | Tot: 1s989ms | Loss: 0.1290 | Acc: 96.000% (2880/3000) [==========================================>.....................................]  Step: 68ms | Tot: 17s780ms | Loss: 0.1651 | Acc: 94.860% (25043/26400) [========================================================>.......................]  Step: 66ms | Tot: 23s763ms | Loss: 0.1653 | Acc: 94.816% (33565/35400) [==========================================================>.....................]  Step: 66ms | Tot: 24s694ms | Loss: 0.1644 | Acc: 94.812% (34891/36800) [===========================================================>....................]  Step: 66ms | Tot: 24s760ms | Loss: 0.1644 | Acc: 94.813% (34986/36900) [===================================================================>............]  Step: 66ms | Tot: 28s335ms | Loss: 0.1683 | Acc: 94.709% (40062/42300)\n",
      "(82.12053819186985, 0.948)\n",
      "\n",
      "===> epoch: 75/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 33s206ms | Loss: 0.1658 | Acc: 94.684% (47342/50000)........................................................................]  Step: 67ms | Tot: 470ms | Loss: 0.1182 | Acc: 96.250% (770/800) [=>..............................................................................]  Step: 66ms | Tot: 536ms | Loss: 0.1594 | Acc: 95.667% (861/900) [=>..............................................................................]  Step: 66ms | Tot: 603ms | Loss: 0.1598 | Acc: 95.500% (955/1000) [=====================>..........................................................]  Step: 67ms | Tot: 9s79ms | Loss: 0.1545 | Acc: 95.051% (13022/13700) [==================================>.............................................]  Step: 68ms | Tot: 14s214ms | Loss: 0.1534 | Acc: 95.112% (20354/21400) [=====================================>..........................................]  Step: 66ms | Tot: 15s580ms | Loss: 0.1530 | Acc: 95.137% (22262/23400) [======================================>.........................................]  Step: 67ms | Tot: 15s848ms | Loss: 0.1525 | Acc: 95.151% (22646/23800) [============================================>...................................]  Step: 66ms | Tot: 18s659ms | Loss: 0.1594 | Acc: 94.961% (26589/28000) [====================================================>...........................]  Step: 66ms | Tot: 21s966ms | Loss: 0.1608 | Acc: 94.888% (31313/33000) [=====================================================>..........................]  Step: 68ms | Tot: 22s100ms | Loss: 0.1611 | Acc: 94.880% (31500/33200) [===============================================================================>]  Step: 66ms | Tot: 33s8ms | Loss: 0.1655 | Acc: 94.704% (47068/49700)\n",
      "(82.92325703799725, 0.94684)\n",
      "\n",
      "===> epoch: 76/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 33s217ms | Loss: 0.0963 | Acc: 96.868% (48434/50000)=========>...................................................................]  Step: 66ms | Tot: 5s347ms | Loss: 0.1137 | Acc: 96.138% (7691/8000) [=============>..................................................................]  Step: 67ms | Tot: 5s680ms | Loss: 0.1111 | Acc: 96.271% (8183/8500) [==============>.................................................................]  Step: 65ms | Tot: 6s227ms | Loss: 0.1103 | Acc: 96.269% (8953/9300) [=======================>........................................................]  Step: 67ms | Tot: 9s682ms | Loss: 0.1034 | Acc: 96.542% (13902/14400) [============================>...................................................]  Step: 66ms | Tot: 11s826ms | Loss: 0.1014 | Acc: 96.611% (16907/17500) [================================>...............................................]  Step: 67ms | Tot: 13s500ms | Loss: 0.0993 | Acc: 96.735% (19347/20000)\n",
      "(48.153314552269876, 0.96868)\n",
      "\n",
      "===> epoch: 77/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s121ms | Loss: 0.0727 | Acc: 97.648% (48824/50000)=====================>..........................................................]  Step: 67ms | Tot: 8s793ms | Loss: 0.0737 | Acc: 97.714% (12996/13300) [=================================>..............................................]  Step: 66ms | Tot: 13s945ms | Loss: 0.0699 | Acc: 97.743% (20526/21000) [===================================================>............................]  Step: 67ms | Tot: 21s313ms | Loss: 0.0720 | Acc: 97.699% (31459/32200) [===========================================================>....................]  Step: 66ms | Tot: 24s628ms | Loss: 0.0723 | Acc: 97.671% (36236/37100) [===========================================================>....................]  Step: 66ms | Tot: 24s695ms | Loss: 0.0722 | Acc: 97.669% (36333/37200) [============================================================>...................]  Step: 67ms | Tot: 24s896ms | Loss: 0.0721 | Acc: 97.672% (36627/37500)\n",
      "(36.3407988245599, 0.97648)\n",
      "\n",
      "===> epoch: 78/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s641ms | Loss: 0.0640 | Acc: 97.876% (48938/50000).......................................................................]  Step: 66ms | Tot: 132ms | Loss: 0.0618 | Acc: 98.333% (295/300) [===========================>....................................................]  Step: 71ms | Tot: 11s219ms | Loss: 0.0644 | Acc: 97.851% (17026/17400) [===========================================>....................................]  Step: 67ms | Tot: 17s629ms | Loss: 0.0636 | Acc: 97.930% (26735/27300) [===========================================>....................................]  Step: 67ms | Tot: 17s696ms | Loss: 0.0636 | Acc: 97.934% (26834/27400) [===============================================================>................]  Step: 67ms | Tot: 25s996ms | Loss: 0.0618 | Acc: 97.980% (39094/39900) [================================================================>...............]  Step: 66ms | Tot: 26s330ms | Loss: 0.0618 | Acc: 97.975% (39582/40400) [==============================================================================>.]  Step: 66ms | Tot: 32s39ms | Loss: 0.0637 | Acc: 97.894% (48066/49100) [===============================================================================>]  Step: 66ms | Tot: 32s241ms | Loss: 0.0637 | Acc: 97.893% (48359/49400)\n",
      "(31.998061415739357, 0.97876)\n",
      "\n",
      "===> epoch: 79/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s573ms | Loss: 0.0679 | Acc: 97.778% (48889/50000)........................................................................]  Step: 67ms | Tot: 464ms | Loss: 0.0832 | Acc: 97.125% (777/800) [=>..............................................................................]  Step: 66ms | Tot: 732ms | Loss: 0.0864 | Acc: 96.917% (1163/1200) [===>............................................................................]  Step: 66ms | Tot: 1s334ms | Loss: 0.0768 | Acc: 97.619% (2050/2100) [=====>..........................................................................]  Step: 66ms | Tot: 2s129ms | Loss: 0.0871 | Acc: 97.212% (3208/3300) [======>.........................................................................]  Step: 66ms | Tot: 2s726ms | Loss: 0.0813 | Acc: 97.333% (4088/4200) [=========>......................................................................]  Step: 66ms | Tot: 3s997ms | Loss: 0.0716 | Acc: 97.639% (5956/6100) [=========================================>......................................]  Step: 66ms | Tot: 16s939ms | Loss: 0.0646 | Acc: 97.881% (25449/26000) [==========================================>.....................................]  Step: 66ms | Tot: 17s140ms | Loss: 0.0652 | Acc: 97.871% (25740/26300) [==========================================>.....................................]  Step: 66ms | Tot: 17s274ms | Loss: 0.0649 | Acc: 97.879% (25938/26500) [==========================================>.....................................]  Step: 66ms | Tot: 17s340ms | Loss: 0.0654 | Acc: 97.868% (26033/26600) [============================================================================>...]  Step: 66ms | Tot: 31s339ms | Loss: 0.0679 | Acc: 97.788% (47036/48100)\n",
      "(33.96489868685603, 0.97778)\n",
      "\n",
      "===> epoch: 80/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s645ms | Loss: 0.0634 | Acc: 97.904% (48952/50000)===================>............................................................]  Step: 66ms | Tot: 7s966ms | Loss: 0.0541 | Acc: 98.238% (11985/12200) [=======================>........................................................]  Step: 66ms | Tot: 9s638ms | Loss: 0.0582 | Acc: 98.082% (14418/14700) [=======================>........................................................]  Step: 67ms | Tot: 9s706ms | Loss: 0.0580 | Acc: 98.095% (14518/14800)\n",
      "(31.724329428281635, 0.97904)\n",
      "\n",
      "===> epoch: 81/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s455ms | Loss: 0.0593 | Acc: 98.110% (49055/50000)>............................................................................]  Step: 68ms | Tot: 1s517ms | Loss: 0.0528 | Acc: 98.125% (2355/2400) [======>.........................................................................]  Step: 71ms | Tot: 2s767ms | Loss: 0.0500 | Acc: 98.372% (4230/4300) [=========================>......................................................]  Step: 66ms | Tot: 10s267ms | Loss: 0.0494 | Acc: 98.427% (15453/15700)\n",
      "(29.64828714262694, 0.9811)\n",
      "\n",
      "===> epoch: 82/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s549ms | Loss: 0.0593 | Acc: 98.124% (49062/50000)=========>...................................................................]  Step: 66ms | Tot: 4s901ms | Loss: 0.0538 | Acc: 98.213% (7366/7500) [=================================================>..............................]  Step: 68ms | Tot: 20s79ms | Loss: 0.0537 | Acc: 98.288% (30371/30900) [=================================================>..............................]  Step: 66ms | Tot: 20s145ms | Loss: 0.0537 | Acc: 98.287% (30469/31000) [=============================================================>..................]  Step: 67ms | Tot: 24s977ms | Loss: 0.0548 | Acc: 98.263% (37733/38400) [=============================================================>..................]  Step: 67ms | Tot: 25s179ms | Loss: 0.0546 | Acc: 98.271% (38031/38700)\n",
      "(29.65847035101615, 0.98124)\n",
      "\n",
      "===> epoch: 83/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s893ms | Loss: 0.0547 | Acc: 98.228% (49114/50000)=======>.....................................................................]  Step: 67ms | Tot: 4s454ms | Loss: 0.0413 | Acc: 98.632% (6707/6800) [==========================>.....................................................]  Step: 66ms | Tot: 11s51ms | Loss: 0.0462 | Acc: 98.503% (16056/16300) [===========================>....................................................]  Step: 67ms | Tot: 11s660ms | Loss: 0.0479 | Acc: 98.453% (16934/17200) [===============================================>................................]  Step: 65ms | Tot: 19s944ms | Loss: 0.0499 | Acc: 98.369% (29019/29500) [======================================================>.........................]  Step: 68ms | Tot: 23s275ms | Loss: 0.0513 | Acc: 98.341% (33731/34300) [=================================================================>..............]  Step: 66ms | Tot: 27s698ms | Loss: 0.0523 | Acc: 98.331% (40119/40800) [=====================================================================>..........]  Step: 67ms | Tot: 29s340ms | Loss: 0.0543 | Acc: 98.278% (42456/43200) [=========================================================================>......]  Step: 67ms | Tot: 31s98ms | Loss: 0.0541 | Acc: 98.275% (45010/45800) [==============================================================================>.]  Step: 67ms | Tot: 33s359ms | Loss: 0.0547 | Acc: 98.240% (48334/49200)\n",
      "(27.328647621674463, 0.98228)\n",
      "\n",
      "===> epoch: 84/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 69ms | Tot: 33s600ms | Loss: 0.0554 | Acc: 98.240% (49120/50000)==========>..................................................................]  Step: 67ms | Tot: 5s594ms | Loss: 0.0476 | Acc: 98.588% (8380/8500) [==================>.............................................................]  Step: 66ms | Tot: 7s674ms | Loss: 0.0453 | Acc: 98.578% (11435/11600) [=======================>........................................................]  Step: 67ms | Tot: 9s706ms | Loss: 0.0463 | Acc: 98.541% (14387/14600) [=============================>..................................................]  Step: 66ms | Tot: 12s243ms | Loss: 0.0477 | Acc: 98.511% (18126/18400) [==================================>.............................................]  Step: 66ms | Tot: 14s522ms | Loss: 0.0480 | Acc: 98.468% (21466/21800) [==============================================>.................................]  Step: 67ms | Tot: 19s246ms | Loss: 0.0538 | Acc: 98.298% (28408/28900) [===================================================>............................]  Step: 68ms | Tot: 21s587ms | Loss: 0.0531 | Acc: 98.319% (31757/32300) [=====================================================>..........................]  Step: 67ms | Tot: 22s525ms | Loss: 0.0529 | Acc: 98.312% (33131/33700) [==============================================================>.................]  Step: 65ms | Tot: 26s229ms | Loss: 0.0545 | Acc: 98.278% (38525/39200) [==============================================================>.................]  Step: 69ms | Tot: 26s298ms | Loss: 0.0548 | Acc: 98.270% (38620/39300) [===============================================================>................]  Step: 67ms | Tot: 26s567ms | Loss: 0.0549 | Acc: 98.267% (39012/39700) [============================================================================>...]  Step: 67ms | Tot: 32s179ms | Loss: 0.0554 | Acc: 98.255% (47064/47900)\n",
      "(27.69303829059936, 0.9824)\n",
      "\n",
      "===> epoch: 85/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s864ms | Loss: 0.0601 | Acc: 98.120% (49060/50000)[==============================>.................................................]  Step: 66ms | Tot: 12s516ms | Loss: 0.0538 | Acc: 98.284% (18674/19000) [======================================>.........................................]  Step: 66ms | Tot: 15s805ms | Loss: 0.0558 | Acc: 98.242% (23578/24000) [============================================================>...................]  Step: 66ms | Tot: 24s748ms | Loss: 0.0595 | Acc: 98.152% (36905/37600)\n",
      "(30.032435784698464, 0.9812)\n",
      "\n",
      "===> epoch: 86/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 75ms | Tot: 32s882ms | Loss: 0.0507 | Acc: 98.318% (49159/50000)........................................................................]  Step: 68ms | Tot: 397ms | Loss: 0.0754 | Acc: 97.286% (681/700) [=>..............................................................................]  Step: 66ms | Tot: 530ms | Loss: 0.0823 | Acc: 97.444% (877/900) [=>..............................................................................]  Step: 68ms | Tot: 665ms | Loss: 0.0849 | Acc: 97.364% (1071/1100) [==========================================================>.....................]  Step: 66ms | Tot: 24s158ms | Loss: 0.0480 | Acc: 98.417% (36119/36700)\n",
      "(25.33115508512128, 0.98318)\n",
      "\n",
      "===> epoch: 87/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s707ms | Loss: 0.0530 | Acc: 98.332% (49166/50000)[============================================================================>...]  Step: 66ms | Tot: 31s202ms | Loss: 0.0526 | Acc: 98.352% (46914/47700)\n",
      "(26.523230726015754, 0.98332)\n",
      "\n",
      "===> epoch: 88/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 32s939ms | Loss: 0.0485 | Acc: 98.518% (49259/50000)=======>.....................................................................]  Step: 67ms | Tot: 4s381ms | Loss: 0.0458 | Acc: 98.561% (6505/6600) [==========>.....................................................................]  Step: 67ms | Tot: 4s449ms | Loss: 0.0455 | Acc: 98.567% (6604/6700) [===========>....................................................................]  Step: 66ms | Tot: 4s582ms | Loss: 0.0454 | Acc: 98.565% (6801/6900)\n",
      "(24.258148611173965, 0.98518)\n",
      "\n",
      "===> epoch: 89/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 32s936ms | Loss: 0.0550 | Acc: 98.278% (49139/50000)[====================================>...........................................]  Step: 67ms | Tot: 15s196ms | Loss: 0.0616 | Acc: 98.147% (22672/23100) [=====================================>..........................................]  Step: 66ms | Tot: 15s262ms | Loss: 0.0615 | Acc: 98.151% (22771/23200)\n",
      "(27.509912015637383, 0.98278)\n",
      "\n",
      "===> epoch: 90/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 33s90ms | Loss: 0.0570 | Acc: 98.198% (49099/50000))[==============================================>.................................]  Step: 66ms | Tot: 18s979ms | Loss: 0.0528 | Acc: 98.372% (28528/29000) [======================================================================>.........]  Step: 66ms | Tot: 29s206ms | Loss: 0.0550 | Acc: 98.247% (43327/44100)\n",
      "(28.497219738783315, 0.98198)\n",
      "\n",
      "===> epoch: 91/100\n",
      "train:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 500/500 [================================================================================>]  Step: 66ms | Tot: 32s918ms | Loss: 0.0504 | Acc: 98.392% (49196/50000)[=========================================================>......................]  Step: 66ms | Tot: 23s683ms | Loss: 0.0543 | Acc: 98.263% (35473/36100) [==========================================================>.....................]  Step: 66ms | Tot: 23s817ms | Loss: 0.0543 | Acc: 98.259% (35668/36300) [==========================================================>.....................]  Step: 66ms | Tot: 23s951ms | Loss: 0.0541 | Acc: 98.266% (35867/36500) [============================================================>...................]  Step: 66ms | Tot: 24s688ms | Loss: 0.0534 | Acc: 98.282% (36954/37600) [====================================================================>...........]  Step: 67ms | Tot: 27s957ms | Loss: 0.0518 | Acc: 98.339% (41794/42500) [=====================================================================>..........]  Step: 66ms | Tot: 28s698ms | Loss: 0.0514 | Acc: 98.360% (42885/43600) [===============================================================================>]  Step: 66ms | Tot: 32s718ms | Loss: 0.0504 | Acc: 98.396% (48903/49700)\n",
      "(25.19743793515954, 0.98392)\n",
      "\n",
      "===> epoch: 92/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s709ms | Loss: 0.0577 | Acc: 98.212% (49106/50000)======>......................................................................]  Step: 66ms | Tot: 3s759ms | Loss: 0.0501 | Acc: 98.328% (5703/5800) [=========================>......................................................]  Step: 67ms | Tot: 10s544ms | Loss: 0.0562 | Acc: 98.118% (15797/16100) [==============================================>.................................]  Step: 68ms | Tot: 19s80ms | Loss: 0.0587 | Acc: 98.127% (28555/29100)\n",
      "(28.838088843622245, 0.98212)\n",
      "\n",
      "===> epoch: 93/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s771ms | Loss: 0.0440 | Acc: 98.626% (49313/50000)================>...............................................................]  Step: 66ms | Tot: 6s801ms | Loss: 0.0494 | Acc: 98.442% (10238/10400) [===================================>............................................]  Step: 67ms | Tot: 14s477ms | Loss: 0.0427 | Acc: 98.680% (21907/22200) [=====================================>..........................................]  Step: 66ms | Tot: 15s310ms | Loss: 0.0422 | Acc: 98.688% (23093/23400)\n",
      "(22.00639972288627, 0.98626)\n",
      "\n",
      "===> epoch: 94/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 64ms | Tot: 33s34ms | Loss: 0.0499 | Acc: 98.444% (49222/50000)[=================>..............................................................]  Step: 65ms | Tot: 7s91ms | Loss: 0.0442 | Acc: 98.725% (10761/10900) [===================>............................................................]  Step: 66ms | Tot: 7s755ms | Loss: 0.0453 | Acc: 98.697% (11745/11900) [========================>.......................................................]  Step: 67ms | Tot: 10s112ms | Loss: 0.0486 | Acc: 98.513% (15171/15400) [========================>.......................................................]  Step: 66ms | Tot: 10s179ms | Loss: 0.0484 | Acc: 98.523% (15271/15500) [========================>.......................................................]  Step: 67ms | Tot: 10s246ms | Loss: 0.0485 | Acc: 98.519% (15369/15600) [==========================>.....................................................]  Step: 68ms | Tot: 10s911ms | Loss: 0.0479 | Acc: 98.542% (16358/16600) [========================================>.......................................]  Step: 66ms | Tot: 16s641ms | Loss: 0.0438 | Acc: 98.632% (24954/25300) [========================================>.......................................]  Step: 66ms | Tot: 16s775ms | Loss: 0.0438 | Acc: 98.627% (25150/25500) [========================================>.......................................]  Step: 68ms | Tot: 16s843ms | Loss: 0.0440 | Acc: 98.617% (25246/25600) [=========================================>......................................]  Step: 68ms | Tot: 17s176ms | Loss: 0.0442 | Acc: 98.605% (25736/26100) [========================================================>.......................]  Step: 65ms | Tot: 23s450ms | Loss: 0.0449 | Acc: 98.590% (35098/35600) [=================================================================>..............]  Step: 66ms | Tot: 27s247ms | Loss: 0.0478 | Acc: 98.505% (40584/41200)\n",
      "(24.951125754858367, 0.98444)\n",
      "\n",
      "===> epoch: 95/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s118ms | Loss: 0.0484 | Acc: 98.462% (49231/50000)===========>.................................................................]  Step: 66ms | Tot: 5s801ms | Loss: 0.0375 | Acc: 98.865% (8799/8900) [=================>..............................................................]  Step: 67ms | Tot: 7s210ms | Loss: 0.0379 | Acc: 98.900% (10879/11000) [===================>............................................................]  Step: 68ms | Tot: 7s885ms | Loss: 0.0377 | Acc: 98.875% (11865/12000) [=================================>..............................................]  Step: 67ms | Tot: 13s864ms | Loss: 0.0359 | Acc: 98.862% (20761/21000) [==================================>.............................................]  Step: 68ms | Tot: 14s66ms | Loss: 0.0361 | Acc: 98.845% (21054/21300) [==================================>.............................................]  Step: 65ms | Tot: 14s132ms | Loss: 0.0361 | Acc: 98.846% (21153/21400) [==================================>.............................................]  Step: 66ms | Tot: 14s198ms | Loss: 0.0361 | Acc: 98.847% (21252/21500) [===================================>............................................]  Step: 66ms | Tot: 14s664ms | Loss: 0.0368 | Acc: 98.824% (21939/22200) [==================================================>.............................]  Step: 66ms | Tot: 20s795ms | Loss: 0.0381 | Acc: 98.790% (31020/31400)\n",
      "(24.208741266396828, 0.98462)\n",
      "\n",
      "===> epoch: 96/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s954ms | Loss: 0.0533 | Acc: 98.402% (49201/50000)........................................................................]  Step: 67ms | Tot: 535ms | Loss: 0.0404 | Acc: 98.889% (890/900) [=====>..........................................................................]  Step: 70ms | Tot: 2s101ms | Loss: 0.0346 | Acc: 99.030% (3268/3300) [========>.......................................................................]  Step: 67ms | Tot: 3s699ms | Loss: 0.0361 | Acc: 98.875% (5537/5600) [=========>......................................................................]  Step: 66ms | Tot: 3s766ms | Loss: 0.0363 | Acc: 98.842% (5634/5700) [=========>......................................................................]  Step: 67ms | Tot: 3s833ms | Loss: 0.0371 | Acc: 98.793% (5730/5800) [==============>.................................................................]  Step: 64ms | Tot: 5s972ms | Loss: 0.0364 | Acc: 98.733% (8886/9000) [=================>..............................................................]  Step: 67ms | Tot: 7s460ms | Loss: 0.0375 | Acc: 98.723% (11057/11200) [=========================================>......................................]  Step: 66ms | Tot: 17s47ms | Loss: 0.0452 | Acc: 98.663% (25455/25800) [====================================================================>...........]  Step: 67ms | Tot: 28s257ms | Loss: 0.0470 | Acc: 98.561% (42184/42800) [====================================================================>...........]  Step: 67ms | Tot: 28s392ms | Loss: 0.0473 | Acc: 98.560% (42381/43000)\n",
      "(26.634027688472997, 0.98402)\n",
      "\n",
      "===> epoch: 97/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 65ms | Tot: 32s909ms | Loss: 0.0525 | Acc: 98.324% (49162/50000)====>........................................................................]  Step: 65ms | Tot: 3s113ms | Loss: 0.0626 | Acc: 97.979% (4605/4700) [================>...............................................................]  Step: 67ms | Tot: 7s50ms | Loss: 0.0603 | Acc: 98.028% (10391/10600) [===============================>................................................]  Step: 66ms | Tot: 12s856ms | Loss: 0.0631 | Acc: 98.072% (19026/19400) [===========================================>....................................]  Step: 65ms | Tot: 18s1ms | Loss: 0.0600 | Acc: 98.158% (26797/27300) [===========================================>....................................]  Step: 65ms | Tot: 18s66ms | Loss: 0.0599 | Acc: 98.161% (26896/27400) [=======================================================================>........]  Step: 67ms | Tot: 29s459ms | Loss: 0.0520 | Acc: 98.327% (43952/44700) [=======================================================================>........]  Step: 65ms | Tot: 29s592ms | Loss: 0.0518 | Acc: 98.332% (44151/44900)\n",
      "(26.25908639107365, 0.98324)\n",
      "\n",
      "===> epoch: 98/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 32s968ms | Loss: 0.0501 | Acc: 98.474% (49237/50000)===>.........................................................................]  Step: 67ms | Tot: 2s657ms | Loss: 0.0462 | Acc: 98.512% (4039/4100) [================>...............................................................]  Step: 66ms | Tot: 6s776ms | Loss: 0.0451 | Acc: 98.680% (10164/10300) [=================>..............................................................]  Step: 68ms | Tot: 7s309ms | Loss: 0.0437 | Acc: 98.721% (10958/11100) [===================>............................................................]  Step: 67ms | Tot: 8s187ms | Loss: 0.0430 | Acc: 98.750% (12245/12400) [====================>...........................................................]  Step: 67ms | Tot: 8s655ms | Loss: 0.0422 | Acc: 98.763% (12938/13100) [=================================================>..............................]  Step: 67ms | Tot: 20s169ms | Loss: 0.0517 | Acc: 98.453% (30225/30700) [==================================================>.............................]  Step: 66ms | Tot: 20s573ms | Loss: 0.0530 | Acc: 98.412% (30803/31300) [===========================================================================>....]  Step: 67ms | Tot: 31s16ms | Loss: 0.0509 | Acc: 98.454% (46372/47100)\n",
      "(25.048805973958224, 0.98474)\n",
      "\n",
      "===> epoch: 99/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s259ms | Loss: 0.0412 | Acc: 98.762% (49381/50000)........................................................................]  Step: 67ms | Tot: 267ms | Loss: 0.0289 | Acc: 98.800% (494/500) [>...............................................................................]  Step: 67ms | Tot: 335ms | Loss: 0.0377 | Acc: 98.667% (592/600) [======>.........................................................................]  Step: 66ms | Tot: 2s615ms | Loss: 0.0297 | Acc: 98.925% (3957/4000) [==================>.............................................................]  Step: 67ms | Tot: 7s564ms | Loss: 0.0373 | Acc: 98.843% (11367/11500) [==================>.............................................................]  Step: 67ms | Tot: 7s767ms | Loss: 0.0382 | Acc: 98.831% (11662/11800) [=================================================>..............................]  Step: 67ms | Tot: 20s495ms | Loss: 0.0407 | Acc: 98.778% (30720/31100) [==================================================>.............................]  Step: 65ms | Tot: 20s628ms | Loss: 0.0407 | Acc: 98.780% (30918/31300) [======================================================>.........................]  Step: 67ms | Tot: 22s388ms | Loss: 0.0427 | Acc: 98.735% (33471/33900) [==========================================================>.....................]  Step: 67ms | Tot: 24s216ms | Loss: 0.0424 | Acc: 98.754% (36144/36600) [============================================================>...................]  Step: 67ms | Tot: 25s92ms | Loss: 0.0420 | Acc: 98.755% (37428/37900) [======================================================================>.........]  Step: 67ms | Tot: 29s367ms | Loss: 0.0409 | Acc: 98.778% (43660/44200) [============================================================================>...]  Step: 67ms | Tot: 31s914ms | Loss: 0.0402 | Acc: 98.783% (47416/48000) [=============================================================================>..]  Step: 68ms | Tot: 32s179ms | Loss: 0.0404 | Acc: 98.783% (47811/48400) [=============================================================================>..]  Step: 66ms | Tot: 32s313ms | Loss: 0.0408 | Acc: 98.780% (48007/48600) [==============================================================================>.]  Step: 67ms | Tot: 32s447ms | Loss: 0.0409 | Acc: 98.777% (48203/48800) [===============================================================================>]  Step: 67ms | Tot: 32s990ms | Loss: 0.0411 | Acc: 98.768% (48989/49600)\n",
      "(20.624821068893652, 0.98762)\n",
      "\n",
      "===> epoch: 100/100\n",
      "train:\n",
      " 500/500 [================================================================================>]  Step: 67ms | Tot: 33s626ms | Loss: 0.0490 | Acc: 98.470% (49235/50000)==========>..................................................................]  Step: 67ms | Tot: 5s837ms | Loss: 0.0653 | Acc: 97.874% (8515/8700) [==============>.................................................................]  Step: 65ms | Tot: 6s235ms | Loss: 0.0647 | Acc: 97.892% (9104/9300) [=====================>..........................................................]  Step: 68ms | Tot: 9s31ms | Loss: 0.0538 | Acc: 98.289% (13269/13500) [========================>.......................................................]  Step: 67ms | Tot: 10s221ms | Loss: 0.0512 | Acc: 98.368% (14952/15200) [========================>.......................................................]  Step: 68ms | Tot: 10s490ms | Loss: 0.0509 | Acc: 98.365% (15345/15600) [==============================================>.................................]  Step: 66ms | Tot: 19s311ms | Loss: 0.0485 | Acc: 98.469% (28359/28800) [====================================================>...........................]  Step: 66ms | Tot: 21s976ms | Loss: 0.0480 | Acc: 98.491% (32305/32800) [====================================================>...........................]  Step: 68ms | Tot: 22s44ms | Loss: 0.0480 | Acc: 98.489% (32403/32900) [===========================================================>....................]  Step: 67ms | Tot: 24s974ms | Loss: 0.0482 | Acc: 98.538% (36656/37200) [=========================================================================>......]  Step: 66ms | Tot: 30s877ms | Loss: 0.0492 | Acc: 98.468% (45197/45900) [============================================================================>...]  Step: 66ms | Tot: 32s291ms | Loss: 0.0488 | Acc: 98.475% (47268/48000) [============================================================================>...]  Step: 68ms | Tot: 32s359ms | Loss: 0.0487 | Acc: 98.476% (47367/48100) [==============================================================================>.]  Step: 67ms | Tot: 32s894ms | Loss: 0.0489 | Acc: 98.468% (48151/48900)\n",
      "(24.51909423584584, 0.9847)\n",
      "test:\n",
      " 100/100 [================================================================================>]  Step: 12ms | Tot: 1s282ms | Loss: inf | Acc: 73.320% (7332/10000)\n",
      "===> BEST ACC. PERFORMANCE: 73.320%\n",
      "Checkpoint saved to model.pth\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92d57d",
   "metadata": {},
   "source": [
    "After training, the result shows that the best accuracy performance is 73.320%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c84a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6708e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(input,epsilon,data_grad):\n",
    "    pert_out = input + epsilon*data_grad.sign()\n",
    "    pert_out = torch.clamp(pert_out, 0, 1)\n",
    "    return pert_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f77cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,device,test_loader,epsilon,attack):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data.requires_grad = True\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] \n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "        loss = F.nll_loss(output, target)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        if attack == \"fgsm\":\n",
    "            perturbed_data = fgsm_attack(data,epsilon,data_grad)\n",
    "        elif attack == \"austin\":\n",
    "            perturbed_data = austin_attack(data)\n",
    "        elif attack == \"arthur\":\n",
    "            perturbed_data = arthur_attack(data,epsilon,data_grad)\n",
    "\n",
    "        output = model(perturbed_data)\n",
    "        final_pred = output.max(1, keepdim=True)[1]\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "            if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "        else:\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n",
    "\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    if epsilon:\n",
    "        print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "    else:\n",
    "        print(\"Test Accuracy = {} / {} = {}\".format(correct, len(test_loader), final_acc))\n",
    "\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f7e5f",
   "metadata": {},
   "source": [
    "Here we chose epsilons as 0, 0.007, 0.01, 0.02, 0.03, 0.05, 0.1, 0.2, and 0.3 since the accuracy dramatically dropped between 0 to 0.05. And after 0.05, the accuracy flattened. We chose several epsilon values, such as 0.1, 0.2, and 0.3, to show our observation and conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aac498ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 7332 / 10000 = 0.7332\n",
      "Epsilon: 0.005\tTest Accuracy = 3649 / 10000 = 0.3649\n",
      "Epsilon: 0.01\tTest Accuracy = 2013 / 10000 = 0.2013\n",
      "Epsilon: 0.02\tTest Accuracy = 709 / 10000 = 0.0709\n",
      "Epsilon: 0.03\tTest Accuracy = 319 / 10000 = 0.0319\n",
      "Epsilon: 0.05\tTest Accuracy = 150 / 10000 = 0.015\n",
      "Epsilon: 0.1\tTest Accuracy = 205 / 10000 = 0.0205\n",
      "Epsilon: 0.2\tTest Accuracy = 366 / 10000 = 0.0366\n",
      "Epsilon: 0.3\tTest Accuracy = 502 / 10000 = 0.0502\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilons = [0,0.005, 0.01, 0.02, 0.03, 0.05,0.1,0.2,0.3]\n",
    "accuracies = []\n",
    "examples = []\n",
    "device = torch.device(\"mps\")\n",
    "model = torch.load('./model.pth')\n",
    "for eps in epsilons:\n",
    "    acc, ex = test(model, device, test_loader, eps, \"fgsm\")\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, accuracies, \"*-\")\n",
    "plt.title(\"fgsm attack\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da105748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def austin_attack(input):\n",
    "    pert_out = 1-input\n",
    "    pert_out = torch.clamp(pert_out, 0, 1)\n",
    "    return pert_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6c110",
   "metadata": {},
   "source": [
    "Here we performed an attack and the accuracy dropped from 73.320% to 20.03%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93fd50d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 2003 / 10000 = 0.2003\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = torch.load('./model.pth')\n",
    "acc, ex = test(model, device, test_loader, [], \"austin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d52afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arthur_attack(input,epsilon,data_grad):\n",
    "    iter=10\n",
    "    decay_factor=1.0\n",
    "    pert_out = input\n",
    "    alpha = epsilon/iter\n",
    "    g=0\n",
    "    for i in range(iter-1):\n",
    "        g = decay_factor*g + data_grad/torch.norm(data_grad,p=1)\n",
    "        pert_out = pert_out + alpha*torch.sign(g)\n",
    "        pert_out = torch.clamp(pert_out, 0, 1)\n",
    "        if torch.norm((pert_out-input),p=float('inf')) > epsilon:\n",
    "            break\n",
    "    return pert_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b20c7",
   "metadata": {},
   "source": [
    "Here we performed another attack that the accuracy dropped from 73.320% to 20.03%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca041e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
